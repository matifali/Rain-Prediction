{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R_D8dbqug8AS"
      },
      "outputs": [],
      "source": [
        "# Library Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "# Custom funcion definitions\n",
        "\n",
        "def process_dates(df):\n",
        "    # Process Dates\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df['Year'] = df['Date'].dt.year \n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df.drop(['Date'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_YES_NO(df):\n",
        "    # Process rain\n",
        "    df.replace({'No': 0, 'Yes': 1}, inplace=True)\n",
        "    # df['RainToday'].replace({'No': 0, 'Yes': 1}, inplace=True)\n",
        "    # df['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Converting and splitting date to Year, Month and Day\n",
        "    df = process_dates(df.copy())\n",
        "    # Converting \"Yes\" and \"No\" to \"0\" and \"1\" respectively\n",
        "    df = process_YES_NO(df)\n",
        "\n",
        "    # Split Numeric and Catogrical Features\n",
        "    numeric_features = list(df.select_dtypes(include=['float64']).columns)\n",
        "    category_features = list(df.select_dtypes(include=['object']).columns)\n",
        "\n",
        "    # Filling NAs\n",
        "    # RainTomorrow: remove NA rows\n",
        "    # RainToday: remove NA rows\n",
        "    df.dropna(subset=['RainTomorrow','RainToday'],inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Columns where NAs will be replaced with middle value of previous and next day data.\n",
        "    mid_cols = ['Sunshine','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Cloud9am','Cloud3pm','Evaporation']\n",
        "\n",
        "    for feature in mid_cols:\n",
        "        # Finding rows that are NaNs but are sandwiched between two valid values\n",
        "        i = df[(df[feature].isnull()) & (df[feature].shift(-1).notnull()) & (df[feature].shift().notnull())]\n",
        "        j = i.index\n",
        "        df.loc[j,feature] = (df.loc[j-1,feature] + df.loc[j+1,feature])/2\n",
        "\n",
        "    # Fill remaining values with zeros\n",
        "    # Columns where NAs will be replaced with zeros under the assumption that no activity is monitored.\n",
        "    zero_cols = ['Sunshine','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Cloud9am','Cloud3pm','Evaporation']\n",
        "    for feature in zero_cols:\n",
        "        df[feature].fillna(0, inplace=True)\n",
        "\n",
        "    # In all other numeric columns NAs will be replaced with median\n",
        "\n",
        "    # Find remaning numeric features\n",
        "    numeric_features = list(set(numeric_features) - set(zero_cols) - set(['RainToday', 'RainTomorrow']))\n",
        "\n",
        "    for feature in numeric_features:\n",
        "        feature_median = df[feature].median()\n",
        "        df[feature].fillna(feature_median, inplace=True)\n",
        "\n",
        "    # NAs in categorical columns will be filled with mode\n",
        "    for feature in category_features:\n",
        "        feature_mod = df[feature].mode()[0]\n",
        "        df[feature].fillna(feature_mod, inplace=True)\n",
        "\n",
        "    #df['WindGustDir'].fillna(df['WindGustDir'].mode()[0], inplace=True)\n",
        "    #df['WindDir9am'].fillna(df['WindDir9am'].mode()[0], inplace=True)\n",
        "    #df['WindDir3pm'].fillna(df['WindDir3pm'].mode()[0], inplace=True)\n",
        "    #df.drop(category_features, axis=1, inplace=True)\n",
        "\n",
        "    # Drop Features with high correlation with each other and those who have low cross correlation with output\n",
        "    drop_featrures = ['Temp3pm','Pressure3pm','Year','Month','Day']\n",
        "    df = df.drop(drop_featrures,axis=1);\n",
        "\n",
        "    # One hot encoding for all categorical features\n",
        "    df = pd.get_dummies(df, columns=category_features)\n",
        "\n",
        "    y = df['RainTomorrow'].copy()\n",
        "\n",
        "    return df, y\n",
        "\n",
        "def cyclic_encode(df, col):\n",
        "    max_val = df[col].max()\n",
        "    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_val)\n",
        "    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_val)\n",
        "    df.drop([col], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def convert_2numpy(X):\n",
        "    X = X.to_numpy()\n",
        "    return X\n",
        "\n",
        "def normalize(X, *args):\n",
        "    if args:\n",
        "        meanX = args[0]\n",
        "        stdX = args[1]\n",
        "    else:\n",
        "        meanX = X.mean(axis=0)\n",
        "        stdX = X.std(axis=0)\n",
        "        stdX[abs(stdX) <= 1e-8] = 1e-8  \n",
        "\n",
        "    Xnorm = (X - meanX)/stdX\n",
        "    return Xnorm, meanX, stdX\n",
        "\n",
        "def append_ones(X):\n",
        "    X = np.c_[np.ones(X.shape[0]),X]\n",
        "    return X\n",
        "\n",
        "def splitTestTrain(X, y, test_percentage, seedVal=42):\n",
        "    portion = int(test_percentage/100*len(y))\n",
        "    \n",
        "    np.random.seed(seedVal)\n",
        "    np.random.RandomState(seedVal)\n",
        "\n",
        "    order = np.random.permutation(len(X))\n",
        "    portion = portion # percent\n",
        "    X_test = X[order[:portion]]\n",
        "    y_test = y[order[:portion]]\n",
        "    X_train = X[order[portion:]]\n",
        "    y_train = y[order[portion:]] \n",
        "    return X_test, y_test, X_train, y_train\n",
        "\n",
        "def NormalEquation(X, y):\n",
        "    X_trans = X.transpose()\n",
        "    W = np.linalg.inv(X_trans.dot(X))\n",
        "    W = W.dot(X_trans)\n",
        "    W = W.dot(y)\n",
        "    return W\n",
        "\n",
        "# Mean Squared Loss\n",
        "def loss_MSE(X, y, W, n):\n",
        "    y_pred = X.dot(W)\n",
        "    error = y_pred-y\n",
        "    MSE = 1/n * np.sum(np.power(error,2))\n",
        "    return MSE\n",
        "\n",
        "# Cross Entropy Loss\n",
        "def loss_CE(h, y):\n",
        "    n = y.shape[0]\n",
        "    h = h.reshape((n,1))\n",
        "    y = y.reshape((n,1))\n",
        "    epsilon = 1e-5\n",
        "    loss = -(1.0/n) * (np.dot((np.log(h+epsilon)).T, y) + np.dot((np.log(1-h+epsilon)).T, (1-y)))\n",
        "    return np.squeeze(loss)\n",
        "    \n",
        "\n",
        "def CE_gradient(X, y, W, n):\n",
        "    h = predict(X, W)\n",
        "    gradient = (1.0/n) * np.dot((h - y).T , X)\n",
        "    return gradient\n",
        "\n",
        "    \n",
        "def gradient_descent(X, y, alpha, lmbda, max_iterations, tol):\n",
        "    n = len(y)\n",
        "    # np.random.seed()\n",
        "    # W = np.random.randn(X.shape[1])\n",
        "    W = np.zeros(X.shape[1])\n",
        "    lossarray = []\n",
        "    accarray = []\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        \n",
        "        #alphan = alpha*(1.0e-04*iteration + 1.0)\n",
        "        # alphan = alpha*(-5.0e-03*iteration + 1.0)\n",
        "        alphan  = alpha\n",
        "        # L1 Regularization\n",
        "        Wn = W - alphan * (CE_gradient(X, y, W, n) + 1.0/n*lmbda*np.abs(W))\n",
        "        \n",
        "        # Stopping Condition\n",
        "        if np.sum(abs(Wn - W)) < tol:\n",
        "            print(\"Tolerance reached.\")\n",
        "            break\n",
        "\n",
        "        W = Wn\n",
        "        h = predict(X, W)\n",
        "        loss = loss_CE(h, y) + 1.0/n*lmbda*np.sum(W)\n",
        "        lossarray.append(loss) # Loss array to collect losses in each iteration\n",
        "        ypred = predict(X, W)\n",
        "        ypred = ypred > 0.5\n",
        "        acc = modelAccuracy(ypred, y, 0)\n",
        "        accarray.append(acc)\n",
        "\n",
        "        # Print loss every 50 iterations\n",
        "        if (iteration+1) % 50 == 0:\n",
        "            print(\"Iteration: %d - Loss: %.8f - Acc: %0.4f\" %(iteration+1, loss, acc))\n",
        "\n",
        "    return W, lossarray, accarray\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def predict(X, W):\n",
        "    return sigmoid(X.dot(W))\n",
        "\n",
        "def modelAccuracy(ypred, y_test, pOut=1):\n",
        "    # y_test = y_test.squeeze\n",
        "    # Model Accuracy\n",
        "    TP = np.sum(np.logical_and(ypred == 1, y_test == 1))\n",
        "    TN = np.sum(np.logical_and(ypred == 0, y_test == 0))\n",
        "    FP = np.sum(np.logical_and(ypred == 1, y_test == 0))\n",
        "    FN = np.sum(np.logical_and(ypred == 0, y_test == 1))\n",
        "    \n",
        "    if pOut == 1:\n",
        "        print(\"True Posoitive: \", TP)\n",
        "        print(\"True Negative:  \", TN)\n",
        "        print(\"False Posoitive:\", FP)\n",
        "        print(\"False Negative: \", FN)\n",
        "        \n",
        "    #Precision = TP/(TP + FP) \n",
        "    #NPV = TN/(TN + FN)\n",
        "    #Sensitivity = TP / (TP + FN)\n",
        "    #Specifity = TN / (TN + FP)\n",
        "    Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    #confusion_matrix = np.array([[TP, FP, Precision],[FN, TN, NPV],[Sensitivity, Specifity, Accuracy]])\n",
        "    # np.set_printoptions(precision=2,suppress=True)\n",
        "    #print(confusion_matrix)\n",
        "    if pOut == 1:\n",
        "        print(\"Model Accuracy: \", Accuracy*100, \" %\")\n",
        "    \n",
        "    return Accuracy*100\n",
        "\n",
        "def Kfold(X_train, y_train, k):\n",
        "    X_train_folds = np.array_split(X_train, k)\n",
        "    y_train_folds = np.array_split(y_train, k)\n",
        "    return X_train_folds, y_train_folds\n",
        "\n",
        "\n",
        "def LogisticRegressionTrain(X_train, y_train, alpha, lmbda, max_iterations, tol=1e-8):\n",
        "    # Using Gradinet Descent\n",
        "    W, loss, acc = gradient_descent(X_train, y_train, alpha, lmbda, max_iterations, tol)\n",
        "    return W, loss, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-5O3mrgij8U"
      },
      "outputs": [],
      "source": [
        "# Loading Data\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/weatherAUS.csv\")\n",
        "df = pd.read_csv(\"weatherAUS.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YVsR_O8lkNcS"
      },
      "outputs": [],
      "source": [
        "# correlation\n",
        "# corr = df.corr()\n",
        "# corr.style.background_gradient(cmap='coolwarm')\n",
        "\n",
        "# plt.figure(figsize = (12, 16))\n",
        "# h = sb.heatmap(corr, cmap='coolwarm', annot=True, cbar=False)\n",
        "# h.set_yticklabels(h.get_yticklabels(), rotation = 0)\n",
        "# h.xaxis.tick_top()\n",
        "# h.set_xticklabels(h.get_xticklabels(), rotation = 90)\n",
        "\n",
        "#h.figure.savefig(YOURPATH, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "poElaUYhipXn"
      },
      "outputs": [],
      "source": [
        "# Corrleation with Output\n",
        "# df2 = process_YES_NO(df)\n",
        "# y = df2['RainTomorrow'].copy()\n",
        "# ycorr = df2.corrwith(y)\n",
        "# print(ycorr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZgHc54IRiucO"
      },
      "outputs": [],
      "source": [
        "# Data Pre-Processing\n",
        "dfp, y = preprocess_data(df)\n",
        "\n",
        "# Encoding Cyclic Features\n",
        "#X = cyclic_encode(X,'Day')\n",
        "#X = cyclic_encode(X,'Month')\n",
        "\n",
        "# Convert Pandas dateframe to numpy\n",
        "X = dfp.drop(['RainTomorrow'], axis=1)\n",
        "X = convert_2numpy(X)\n",
        "y = convert_2numpy(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f7xpqY3qwADB"
      },
      "outputs": [],
      "source": [
        "# Make Feature Matrix (Append ones)\n",
        "X = append_ones(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vP3zDRdbi3Yj"
      },
      "outputs": [],
      "source": [
        "# Splitting data in test, training sets\n",
        "X_test, y_test, X_train, y_train = splitTestTrain(X, y, 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NqJoOj30i63l"
      },
      "outputs": [],
      "source": [
        "# Normalization \n",
        "X_train_norm, meanX, stdX = normalize(X_train)\n",
        "# Noramalizing test data with the meana nd std of training data\n",
        "X_test_norm, *_ = normalize(X_test, meanX, stdX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "jJIl8y2DjD0C",
        "outputId": "e91a716d-8a4c-438a-a31e-fc748ecf92c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lambda  =  1\n",
            "Fold: 1\n",
            "Iteration: 50 - Loss: 0.56457792 - Acc: 73.0124\n",
            "Iteration: 100 - Loss: 0.56377570 - Acc: 73.3650\n",
            "Fold: 2\n",
            "Iteration: 50 - Loss: 0.56331283 - Acc: 73.2373\n",
            "Iteration: 100 - Loss: 0.56236769 - Acc: 73.6216\n",
            "Fold: 3\n",
            "Iteration: 50 - Loss: 0.56299204 - Acc: 73.0914\n",
            "Iteration: 100 - Loss: 0.56207258 - Acc: 73.4123\n",
            "Fold: 4\n",
            "Iteration: 50 - Loss: 0.56310116 - Acc: 73.1739\n",
            "Iteration: 100 - Loss: 0.56222338 - Acc: 73.5315\n",
            "Fold: 5\n",
            "Iteration: 50 - Loss: 0.56411565 - Acc: 73.0965\n",
            "Iteration: 100 - Loss: 0.56326044 - Acc: 73.4580\n",
            "Average Accuracy =  73.40869094694335\n"
          ]
        }
      ],
      "source": [
        "# Training with cross validation\n",
        "alpha = 1\n",
        "lmbda = np.array([1])\n",
        "k = 5 # numberOfFolds\n",
        "X_ , y_ =  Kfold(X_train_norm, y_train, k)\n",
        "model_accuracies = np.ndarray((len(lmbda),k))\n",
        "losses = np.ndarray((len(lmbda),k))\n",
        "models = np.ndarray((len(lmbda),k,X_train_norm.shape[1]))\n",
        "for i in range(len(lmbda)):\n",
        "    print('Lambda  = ',lmbda[i])\n",
        "    for j in range(k):\n",
        "        print('Fold: %d' %(j+1))\n",
        "        X_train_k, X_validate_k = np.concatenate(X_[:j] + X_[j+1:]), X_[j]\n",
        "        y_train_k, y_validate_k = np.concatenate(y_[:j] + y_[j+1:]), y_[j]\n",
        "        model_k, loss_k, acc_k = LogisticRegressionTrain(X_train_k, y_train_k, alpha, lmbda[i], 100)\n",
        "        # models[i][j] = model_k\n",
        "        losses[i][j] = loss_k[-1]\n",
        "        ypred_k = predict(X_validate_k, model_k)\n",
        "        ypred_k = ypred_k > 0.5\n",
        "        model_accuracies[i][j] = modelAccuracy(ypred_k, y_validate_k, 0)\n",
        "    print('Average Accuracy = ',np.mean(model_accuracies[i][:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdwKSB6aRjFj",
        "outputId": "9dcf3fd0-a6cb-4836-c8c1-0394037bd684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tlmbda\t 5-fold cross validation Accuracy\n",
            "[[ 1.         73.40869095]]\n"
          ]
        }
      ],
      "source": [
        "np.mean(model_accuracies,axis=1)\n",
        "print(\"\\tlmbda\\t 5-fold cross validation Accuracy\")\n",
        "print(np.array([lmbda, np.mean(model_accuracies,axis=1)]).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW064DKmGHsX",
        "outputId": "9e8943d1-7e9e-45f2-bdca-ed47a51a0ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 50 - Loss: 0.56375440 - Acc: 73.1104\n",
            "Iteration: 100 - Loss: 0.56287832 - Acc: 73.4736\n",
            "Iteration: 150 - Loss: 0.56269804 - Acc: 73.6319\n",
            "Iteration: 200 - Loss: 0.56263917 - Acc: 73.6806\n",
            "Iteration: 250 - Loss: 0.56261577 - Acc: 73.6857\n",
            "Iteration: 300 - Loss: 0.56260509 - Acc: 73.6989\n",
            "Iteration: 350 - Loss: 0.56259972 - Acc: 73.6999\n",
            "Iteration: 400 - Loss: 0.56259686 - Acc: 73.7070\n",
            "Iteration: 450 - Loss: 0.56259528 - Acc: 73.7040\n",
            "Iteration: 500 - Loss: 0.56259439 - Acc: 73.6979\n",
            "Iteration: 550 - Loss: 0.56259388 - Acc: 73.7050\n",
            "Iteration: 600 - Loss: 0.56259358 - Acc: 73.6979\n",
            "Iteration: 650 - Loss: 0.56259340 - Acc: 73.6969\n",
            "Iteration: 700 - Loss: 0.56259329 - Acc: 73.6918\n",
            "Iteration: 750 - Loss: 0.56259321 - Acc: 73.6918\n",
            "Iteration: 800 - Loss: 0.56259316 - Acc: 73.6918\n",
            "Iteration: 850 - Loss: 0.56259313 - Acc: 73.6877\n",
            "Iteration: 900 - Loss: 0.56259310 - Acc: 73.6877\n",
            "Iteration: 950 - Loss: 0.56259308 - Acc: 73.6867\n",
            "Iteration: 1000 - Loss: 0.56259306 - Acc: 73.6867\n",
            "Iteration: 1050 - Loss: 0.56259304 - Acc: 73.6867\n",
            "Iteration: 1100 - Loss: 0.56259302 - Acc: 73.6867\n",
            "Iteration: 1150 - Loss: 0.56259300 - Acc: 73.6877\n",
            "Iteration: 1200 - Loss: 0.56259298 - Acc: 73.6857\n",
            "Iteration: 1250 - Loss: 0.56259297 - Acc: 73.6857\n",
            "Iteration: 1300 - Loss: 0.56259295 - Acc: 73.6857\n",
            "Iteration: 1350 - Loss: 0.56259293 - Acc: 73.6857\n",
            "Iteration: 1400 - Loss: 0.56259292 - Acc: 73.6857\n",
            "Iteration: 1450 - Loss: 0.56259290 - Acc: 73.6857\n",
            "Iteration: 1500 - Loss: 0.56259289 - Acc: 73.6847\n",
            "Iteration: 1550 - Loss: 0.56259287 - Acc: 73.6847\n",
            "Iteration: 1600 - Loss: 0.56259285 - Acc: 73.6847\n",
            "Iteration: 1650 - Loss: 0.56259284 - Acc: 73.6837\n",
            "Iteration: 1700 - Loss: 0.56259282 - Acc: 73.6837\n",
            "Iteration: 1750 - Loss: 0.56259280 - Acc: 73.6837\n",
            "Iteration: 1800 - Loss: 0.56259279 - Acc: 73.6837\n",
            "Iteration: 1850 - Loss: 0.56259277 - Acc: 73.6837\n",
            "Iteration: 1900 - Loss: 0.56259276 - Acc: 73.6837\n",
            "Iteration: 1950 - Loss: 0.56259274 - Acc: 73.6837\n",
            "Iteration: 2000 - Loss: 0.56259272 - Acc: 73.6837\n",
            "Iteration: 2050 - Loss: 0.56259271 - Acc: 73.6837\n",
            "Iteration: 2100 - Loss: 0.56259269 - Acc: 73.6837\n",
            "Iteration: 2150 - Loss: 0.56259268 - Acc: 73.6837\n",
            "Iteration: 2200 - Loss: 0.56259266 - Acc: 73.6837\n",
            "Iteration: 2250 - Loss: 0.56259264 - Acc: 73.6837\n",
            "Iteration: 2300 - Loss: 0.56259263 - Acc: 73.6837\n",
            "Iteration: 2350 - Loss: 0.56259261 - Acc: 73.6837\n",
            "Iteration: 2400 - Loss: 0.56259259 - Acc: 73.6837\n",
            "Iteration: 2450 - Loss: 0.56259258 - Acc: 73.6837\n",
            "Iteration: 2500 - Loss: 0.56259256 - Acc: 73.6837\n",
            "Iteration: 2550 - Loss: 0.56259255 - Acc: 73.6837\n",
            "Iteration: 2600 - Loss: 0.56259253 - Acc: 73.6837\n",
            "Iteration: 2650 - Loss: 0.56259251 - Acc: 73.6837\n",
            "Iteration: 2700 - Loss: 0.56259250 - Acc: 73.6837\n",
            "Iteration: 2750 - Loss: 0.56259248 - Acc: 73.6837\n",
            "Iteration: 2800 - Loss: 0.56259247 - Acc: 73.6837\n",
            "Iteration: 2850 - Loss: 0.56259245 - Acc: 73.6837\n",
            "Iteration: 2900 - Loss: 0.56259243 - Acc: 73.6837\n",
            "Iteration: 2950 - Loss: 0.56259242 - Acc: 73.6837\n",
            "Iteration: 3000 - Loss: 0.56259240 - Acc: 73.6837\n",
            "Iteration: 3050 - Loss: 0.56259238 - Acc: 73.6837\n",
            "Iteration: 3100 - Loss: 0.56259237 - Acc: 73.6837\n",
            "Iteration: 3150 - Loss: 0.56259235 - Acc: 73.6837\n",
            "Iteration: 3200 - Loss: 0.56259234 - Acc: 73.6837\n",
            "Iteration: 3250 - Loss: 0.56259232 - Acc: 73.6837\n",
            "Iteration: 3300 - Loss: 0.56259230 - Acc: 73.6837\n",
            "Iteration: 3350 - Loss: 0.56259229 - Acc: 73.6837\n",
            "Iteration: 3400 - Loss: 0.56259227 - Acc: 73.6837\n",
            "Iteration: 3450 - Loss: 0.56259226 - Acc: 73.6837\n",
            "Iteration: 3500 - Loss: 0.56259224 - Acc: 73.6837\n",
            "Iteration: 3550 - Loss: 0.56259222 - Acc: 73.6837\n",
            "Iteration: 3600 - Loss: 0.56259221 - Acc: 73.6837\n",
            "Iteration: 3650 - Loss: 0.56259219 - Acc: 73.6837\n",
            "Iteration: 3700 - Loss: 0.56259217 - Acc: 73.6837\n",
            "Iteration: 3750 - Loss: 0.56259216 - Acc: 73.6837\n",
            "Iteration: 3800 - Loss: 0.56259214 - Acc: 73.6837\n",
            "Iteration: 3850 - Loss: 0.56259213 - Acc: 73.6837\n",
            "Iteration: 3900 - Loss: 0.56259211 - Acc: 73.6837\n",
            "Iteration: 3950 - Loss: 0.56259209 - Acc: 73.6837\n",
            "Iteration: 4000 - Loss: 0.56259208 - Acc: 73.6837\n",
            "Iteration: 4050 - Loss: 0.56259206 - Acc: 73.6837\n",
            "Iteration: 4100 - Loss: 0.56259204 - Acc: 73.6837\n",
            "Iteration: 4150 - Loss: 0.56259203 - Acc: 73.6837\n",
            "Iteration: 4200 - Loss: 0.56259201 - Acc: 73.6837\n",
            "Iteration: 4250 - Loss: 0.56259200 - Acc: 73.6837\n",
            "Iteration: 4300 - Loss: 0.56259198 - Acc: 73.6837\n",
            "Iteration: 4350 - Loss: 0.56259196 - Acc: 73.6837\n",
            "Iteration: 4400 - Loss: 0.56259195 - Acc: 73.6837\n",
            "Iteration: 4450 - Loss: 0.56259193 - Acc: 73.6837\n",
            "Iteration: 4500 - Loss: 0.56259192 - Acc: 73.6837\n",
            "Iteration: 4550 - Loss: 0.56259190 - Acc: 73.6837\n",
            "Iteration: 4600 - Loss: 0.56259188 - Acc: 73.6837\n",
            "Iteration: 4650 - Loss: 0.56259187 - Acc: 73.6837\n",
            "Iteration: 4700 - Loss: 0.56259185 - Acc: 73.6837\n",
            "Iteration: 4750 - Loss: 0.56259183 - Acc: 73.6837\n",
            "Iteration: 4800 - Loss: 0.56259182 - Acc: 73.6837\n",
            "Iteration: 4850 - Loss: 0.56259180 - Acc: 73.6837\n",
            "Iteration: 4900 - Loss: 0.56259179 - Acc: 73.6837\n",
            "Iteration: 4950 - Loss: 0.56259177 - Acc: 73.6837\n",
            "Iteration: 5000 - Loss: 0.56259175 - Acc: 73.6837\n",
            "Iteration: 5050 - Loss: 0.56259174 - Acc: 73.6837\n",
            "Iteration: 5100 - Loss: 0.56259172 - Acc: 73.6837\n",
            "Iteration: 5150 - Loss: 0.56259170 - Acc: 73.6837\n",
            "Iteration: 5200 - Loss: 0.56259169 - Acc: 73.6837\n",
            "Iteration: 5250 - Loss: 0.56259167 - Acc: 73.6837\n",
            "Iteration: 5300 - Loss: 0.56259166 - Acc: 73.6837\n",
            "Iteration: 5350 - Loss: 0.56259164 - Acc: 73.6837\n",
            "Iteration: 5400 - Loss: 0.56259162 - Acc: 73.6837\n",
            "Iteration: 5450 - Loss: 0.56259161 - Acc: 73.6837\n",
            "Iteration: 5500 - Loss: 0.56259159 - Acc: 73.6837\n",
            "Iteration: 5550 - Loss: 0.56259158 - Acc: 73.6837\n",
            "Iteration: 5600 - Loss: 0.56259156 - Acc: 73.6837\n",
            "Iteration: 5650 - Loss: 0.56259154 - Acc: 73.6837\n",
            "Iteration: 5700 - Loss: 0.56259153 - Acc: 73.6837\n",
            "Iteration: 5750 - Loss: 0.56259151 - Acc: 73.6837\n",
            "Iteration: 5800 - Loss: 0.56259149 - Acc: 73.6837\n",
            "Iteration: 5850 - Loss: 0.56259148 - Acc: 73.6837\n",
            "Iteration: 5900 - Loss: 0.56259146 - Acc: 73.6837\n",
            "Iteration: 5950 - Loss: 0.56259145 - Acc: 73.6837\n",
            "Iteration: 6000 - Loss: 0.56259143 - Acc: 73.6837\n",
            "Iteration: 6050 - Loss: 0.56259141 - Acc: 73.6837\n",
            "Iteration: 6100 - Loss: 0.56259140 - Acc: 73.6837\n",
            "Iteration: 6150 - Loss: 0.56259138 - Acc: 73.6837\n",
            "Iteration: 6200 - Loss: 0.56259136 - Acc: 73.6837\n",
            "Iteration: 6250 - Loss: 0.56259135 - Acc: 73.6837\n",
            "Iteration: 6300 - Loss: 0.56259133 - Acc: 73.6837\n",
            "Iteration: 6350 - Loss: 0.56259132 - Acc: 73.6837\n",
            "Iteration: 6400 - Loss: 0.56259130 - Acc: 73.6837\n",
            "Iteration: 6450 - Loss: 0.56259128 - Acc: 73.6837\n",
            "Iteration: 6500 - Loss: 0.56259127 - Acc: 73.6837\n",
            "Iteration: 6550 - Loss: 0.56259125 - Acc: 73.6837\n",
            "Iteration: 6600 - Loss: 0.56259123 - Acc: 73.6837\n",
            "Iteration: 6650 - Loss: 0.56259122 - Acc: 73.6837\n",
            "Iteration: 6700 - Loss: 0.56259120 - Acc: 73.6837\n",
            "Iteration: 6750 - Loss: 0.56259119 - Acc: 73.6837\n",
            "Iteration: 6800 - Loss: 0.56259117 - Acc: 73.6837\n",
            "Iteration: 6850 - Loss: 0.56259115 - Acc: 73.6837\n",
            "Iteration: 6900 - Loss: 0.56259114 - Acc: 73.6837\n",
            "Iteration: 6950 - Loss: 0.56259112 - Acc: 73.6837\n",
            "Iteration: 7000 - Loss: 0.56259110 - Acc: 73.6837\n",
            "Iteration: 7050 - Loss: 0.56259109 - Acc: 73.6837\n",
            "Iteration: 7100 - Loss: 0.56259107 - Acc: 73.6837\n",
            "Iteration: 7150 - Loss: 0.56259106 - Acc: 73.6837\n",
            "Iteration: 7200 - Loss: 0.56259104 - Acc: 73.6837\n",
            "Iteration: 7250 - Loss: 0.56259102 - Acc: 73.6837\n",
            "Iteration: 7300 - Loss: 0.56259101 - Acc: 73.6837\n",
            "Iteration: 7350 - Loss: 0.56259099 - Acc: 73.6837\n",
            "Iteration: 7400 - Loss: 0.56259097 - Acc: 73.6837\n",
            "Iteration: 7450 - Loss: 0.56259096 - Acc: 73.6837\n",
            "Iteration: 7500 - Loss: 0.56259094 - Acc: 73.6837\n",
            "Iteration: 7550 - Loss: 0.56259093 - Acc: 73.6837\n",
            "Iteration: 7600 - Loss: 0.56259091 - Acc: 73.6837\n",
            "Iteration: 7650 - Loss: 0.56259089 - Acc: 73.6837\n",
            "Iteration: 7700 - Loss: 0.56259088 - Acc: 73.6837\n",
            "Iteration: 7750 - Loss: 0.56259086 - Acc: 73.6837\n",
            "Iteration: 7800 - Loss: 0.56259084 - Acc: 73.6837\n",
            "Iteration: 7850 - Loss: 0.56259083 - Acc: 73.6837\n",
            "Iteration: 7900 - Loss: 0.56259081 - Acc: 73.6837\n",
            "Iteration: 7950 - Loss: 0.56259080 - Acc: 73.6837\n",
            "Iteration: 8000 - Loss: 0.56259078 - Acc: 73.6837\n",
            "Iteration: 8050 - Loss: 0.56259076 - Acc: 73.6837\n",
            "Iteration: 8100 - Loss: 0.56259075 - Acc: 73.6837\n",
            "Iteration: 8150 - Loss: 0.56259073 - Acc: 73.6837\n",
            "Iteration: 8200 - Loss: 0.56259071 - Acc: 73.6837\n",
            "Iteration: 8250 - Loss: 0.56259070 - Acc: 73.6837\n",
            "Iteration: 8300 - Loss: 0.56259068 - Acc: 73.6837\n",
            "Iteration: 8350 - Loss: 0.56259067 - Acc: 73.6837\n",
            "Iteration: 8400 - Loss: 0.56259065 - Acc: 73.6837\n",
            "Iteration: 8450 - Loss: 0.56259063 - Acc: 73.6837\n",
            "Iteration: 8500 - Loss: 0.56259062 - Acc: 73.6837\n",
            "Iteration: 8550 - Loss: 0.56259060 - Acc: 73.6837\n",
            "Iteration: 8600 - Loss: 0.56259058 - Acc: 73.6837\n",
            "Iteration: 8650 - Loss: 0.56259057 - Acc: 73.6837\n",
            "Iteration: 8700 - Loss: 0.56259055 - Acc: 73.6837\n",
            "Iteration: 8750 - Loss: 0.56259053 - Acc: 73.6837\n",
            "Iteration: 8800 - Loss: 0.56259052 - Acc: 73.6837\n",
            "Iteration: 8850 - Loss: 0.56259050 - Acc: 73.6837\n",
            "Iteration: 8900 - Loss: 0.56259049 - Acc: 73.6837\n",
            "Iteration: 8950 - Loss: 0.56259047 - Acc: 73.6837\n",
            "Iteration: 9000 - Loss: 0.56259045 - Acc: 73.6837\n",
            "Iteration: 9050 - Loss: 0.56259044 - Acc: 73.6837\n",
            "Iteration: 9100 - Loss: 0.56259042 - Acc: 73.6837\n",
            "Iteration: 9150 - Loss: 0.56259040 - Acc: 73.6837\n",
            "Iteration: 9200 - Loss: 0.56259039 - Acc: 73.6837\n",
            "Iteration: 9250 - Loss: 0.56259037 - Acc: 73.6837\n",
            "Iteration: 9300 - Loss: 0.56259036 - Acc: 73.6837\n",
            "Iteration: 9350 - Loss: 0.56259034 - Acc: 73.6837\n",
            "Iteration: 9400 - Loss: 0.56259032 - Acc: 73.6837\n",
            "Iteration: 9450 - Loss: 0.56259031 - Acc: 73.6837\n",
            "Iteration: 9500 - Loss: 0.56259029 - Acc: 73.6837\n",
            "Iteration: 9550 - Loss: 0.56259027 - Acc: 73.6837\n",
            "Iteration: 9600 - Loss: 0.56259026 - Acc: 73.6837\n",
            "Iteration: 9650 - Loss: 0.56259024 - Acc: 73.6837\n",
            "Iteration: 9700 - Loss: 0.56259023 - Acc: 73.6837\n",
            "Iteration: 9750 - Loss: 0.56259021 - Acc: 73.6837\n",
            "Iteration: 9800 - Loss: 0.56259019 - Acc: 73.6837\n",
            "Iteration: 9850 - Loss: 0.56259018 - Acc: 73.6837\n",
            "Iteration: 9900 - Loss: 0.56259016 - Acc: 73.6837\n",
            "Iteration: 9950 - Loss: 0.56259014 - Acc: 73.6837\n",
            "Iteration: 10000 - Loss: 0.56259013 - Acc: 73.6837\n",
            "True Posoitive:  18197\n",
            "True Negative:   54419\n",
            "False Posoitive: 22348\n",
            "False Negative:  3587\n",
            "Model Accuracy:  73.68367647208045  %\n",
            "Training Accuracy =  73.68367647208045\n"
          ]
        }
      ],
      "source": [
        "# Traning with Final parameters\n",
        "lmbda = 1.0\n",
        "alpha = 1.0\n",
        "model, loss, acc = LogisticRegressionTrain(X_train_norm, y_train, alpha, lmbda, 10000)\n",
        "ypred = predict(X_train_norm, model)\n",
        "ypred = ypred > 0.5\n",
        "print('Training Accuracy = ', modelAccuracy(ypred, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "SnIHrjeEjGuX",
        "outputId": "40f0d7b7-215f-4c05-d6a4-4ce4231a9f8c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting Training Loss\n",
        "# plt.plot(loss, label='alpha = 1e-4(1e-4 * ietartion + 1.0)')\n",
        "plt.plot(loss)\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Loss')\n",
        "# plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXjoIwjpHs7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srjxrDJCjFbi",
        "outputId": "273bfdcd-c732-4130-b81a-f289be409cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00000000e+00 -8.65798614e-02  7.61201775e-02  4.50092150e-01\n",
            "  3.76369977e-02 -2.44558017e-01  5.23761700e-01  1.28761227e-02\n",
            " -1.88122597e-01 -6.06766408e-02  9.55432114e-01 -2.76449500e-01\n",
            "  7.98287344e-04  1.71955822e-01  1.49671314e-02  7.85146139e-02\n",
            "  7.30344634e-02  1.03595689e-01  2.26779426e-03  7.87100108e-02\n",
            " -5.53649924e-03 -8.04003252e-02 -3.14533892e-02  4.19124230e-02\n",
            "  1.65876275e-03 -3.62126086e-03  2.59212040e-02 -1.63715278e-02\n",
            "  2.99227654e-02 -6.23384709e-03 -4.32429626e-02 -6.21065813e-02\n",
            " -6.21980355e-02 -8.54063967e-02 -8.30574575e-03 -2.80345168e-02\n",
            "  6.04831199e-02  3.82205111e-02  1.38111851e-02 -1.12992210e-01\n",
            "  3.43884456e-02 -9.34101289e-03 -7.03624170e-02 -5.27187180e-02\n",
            "  3.31299534e-02  5.93100983e-02  5.75380517e-03  6.32613250e-02\n",
            "  5.50629557e-02 -7.63522571e-03 -9.29718914e-03 -4.43815362e-02\n",
            "  1.36804456e-02  6.07650720e-02  1.37454603e-02 -5.40122053e-02\n",
            " -6.82358599e-03  3.22247884e-02  4.25735700e-02 -2.08310788e-02\n",
            " -9.84516698e-03 -4.42509566e-03  8.74918544e-03 -1.22702044e-01\n",
            "  4.73865514e-02 -1.41737053e-02 -2.06665191e-02 -3.76521021e-03\n",
            " -3.76657293e-03 -3.46908519e-02 -3.14014676e-02  4.52141796e-03\n",
            "  6.21748039e-03 -8.68216723e-03 -4.15932896e-03 -8.26609065e-04\n",
            " -1.50595959e-02 -1.28046356e-02  9.80322539e-02  1.11302075e-04\n",
            " -7.74271515e-03  1.04778099e-02  3.34865617e-02 -2.28490270e-03\n",
            "  4.09988609e-02  2.69743744e-02  5.25246404e-02  1.56737852e-02\n",
            "  2.57789565e-03 -4.04044810e-02 -1.46915922e-02 -3.10830828e-02\n",
            " -2.72152319e-02 -2.45475150e-02 -2.74280064e-02 -8.04826532e-03\n",
            " -2.69961163e-02 -8.06270492e-03  7.17460092e-03 -1.12067665e-02\n",
            "  6.39186460e-02 -4.69623650e-03  4.11925312e-02  8.10263399e-02\n",
            "  7.14673693e-02 -2.75957263e-02 -3.75339564e-03 -3.83118959e-02\n",
            " -2.50769566e-02 -5.37519173e-02 -3.56632406e-02  1.17191336e-02\n",
            " -5.02698059e-02]\n",
            "True Posoitive:  7900\n",
            "True Negative:   23166\n",
            "False Posoitive: 9653\n",
            "False Negative:  1517\n",
            "Model Accuracy:  73.55336679609812  %\n"
          ]
        }
      ],
      "source": [
        "# Model Testing\n",
        "print(model)\n",
        "ypred_LR = predict(X_test_norm, model)\n",
        "ypred_LR = ypred_LR > 0.5\n",
        "LogisticRegressionTestAcuracy = modelAccuracy(ypred_LR, y_test);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f_r0b94DOkdV"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes Classifier\n",
        "# Split the dataset by true and false classes\n",
        "def separate_by_class(X,y):\n",
        "    I_true  = np.where(y == 1)\n",
        "    X_true  = np.squeeze(X[I_true,:])\n",
        "    I_false = np.where(y == 0)\n",
        "    X_false = np.squeeze(X[I_false,:])\n",
        "    return X_true, X_false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DGKNPz_bq0bO"
      },
      "outputs": [],
      "source": [
        "# Calculate Log Prior Probabilities\n",
        "def log_prior_NB(y):\n",
        "    P_y_true = np.log(np.sum(y)/len(y))\n",
        "    P_y_false = np.log((len(y) - np.sum(y))/len(y))\n",
        "    return np.array([P_y_true, P_y_false])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LU2u_Z02sI3c"
      },
      "outputs": [],
      "source": [
        "# Distribution Parameters\n",
        "def parameters_NB(X,y):\n",
        "    X_true, X_false = separate_by_class(X,y)\n",
        "    meanX = np.vstack((X_true.mean(axis=0), X_false.mean(axis=0)))\n",
        "    stdX = np.vstack((X_true.std(axis=0), X_false.std(axis=0)))\n",
        "    stdX[stdX==0] = 1e-8;\n",
        "    return  meanX, stdX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AFWxNgCKW4Ln"
      },
      "outputs": [],
      "source": [
        "# Calculate Log Likelihhod Probabilities\n",
        "def calculate_log_probability(X, meanX, stdX):\n",
        "    exponent = np.exp(-((X-meanX)**2 / (2 * stdX**2 )))\n",
        "    return np.log(exponent / (np.sqrt(2 * np.pi)* stdX))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rDrBfiMYqISf"
      },
      "outputs": [],
      "source": [
        "# Predict Labels\n",
        "def predict_NB(X):\n",
        "\n",
        "    meanX, stdX = parameters_NB(X_train,y_train)\n",
        "    log_prior_y = log_prior_NB(y_train);\n",
        "    P_y_true_given_X = np.squeeze(np.ndarray((1,len(X))))\n",
        "    P_y_false_given_X = np.squeeze(np.ndarray((1,len(X))))\n",
        "    y_NB = np.squeeze(np.ndarray((1,len(X))))\n",
        "    for i in range(len(X) - 1):\n",
        "        P_y_true_given_X[i] = np.sum(calculate_log_probability(X[i,:], meanX[0,:], stdX[0,:])) + log_prior_y[0]\n",
        "        P_y_false_given_X[i] = np.sum(calculate_log_probability(X[i,:], meanX[1,:], stdX[1,:])) + log_prior_y[1]\n",
        "        if (P_y_true_given_X[i] > P_y_false_given_X[i]):\n",
        "            y_NB[i] = 1\n",
        "        else:\n",
        "            y_NB[i] = 0          \n",
        "    return y_NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fg1Dg9jmFpL",
        "outputId": "44f6b6ad-90e6-49bd-9b33-f283a6fc419c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_23223/1828221285.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  return np.log(exponent / (np.sqrt(2 * np.pi)* stdX))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Posoitive:  6666\n",
            "True Negative:   20109\n",
            "False Posoitive: 12709\n",
            "False Negative:  2751\n",
            "Model Accuracy:  63.39528826802415  %\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "y_NB = predict_NB(X_test)\n",
        "NaiveBayesTestAccuracy = modelAccuracy(y_NB, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rtuGY8kZ5bXt"
      },
      "outputs": [],
      "source": [
        "# Multilayer Perceptron\n",
        " \n",
        "def forward_pass(X, w_1, w_2, w_o):\n",
        "    # hidden layer 1\n",
        "    z_1 = X.dot(w_1)    # input from inpot layer\n",
        "    a_1 = sigmoid(z_1)  # output to hidden layer 2\n",
        "     \n",
        "    # hidden layer 2\n",
        "    z_2 = a_1.dot(w_2)  # input from hidden layer 1\n",
        "    a_2 = sigmoid(z_2)  # output to output layer\n",
        "\n",
        "    # Output layer\n",
        "    z_o = a_2.dot(w_o)  # input from hidden layer 2\n",
        "    a_o = sigmoid(z_o)  # output\n",
        "    return(a_o)\n",
        "  \n",
        "# initializing the weights randomly\n",
        "def initialize_wt(x, y):\n",
        "    l =[]\n",
        "    for i in range(x * y):\n",
        "        l.append(np.random.randn())\n",
        "    return(np.array(l).reshape(x, y))\n",
        "\n",
        "# Back propagation of error\n",
        "def back_pass(X, y, w_1, w_2, w_o, alpha):\n",
        "    y = y.reshape((len(y),1))\n",
        "    # hidden layer 1\n",
        "    z_1 = X.dot(w_1)    # input from inpot layer\n",
        "    a_1 = sigmoid(z_1)  # output to hidden layer 2\n",
        "     \n",
        "    # hidden layer 2\n",
        "    z_2 = a_1.dot(w_2)  # input from hidden layer 1\n",
        "    a_2 = sigmoid(z_2)  # output to output layer\n",
        "\n",
        "    # Output layer\n",
        "    z_o = a_2.dot(w_o)  # input from hidden layer 2\n",
        "    a_o = sigmoid(z_o)  # output\n",
        "\n",
        "    # deltas\n",
        "    d_o = a_o - y       # output layer\n",
        "    # d_2 = (a_2*(1-a_2)).dot(w_o) * d_o\n",
        "    # d_1 = (a_1*(1-a_1)).dot(w_2) * d_2\n",
        "    d_2 = (a_2*(1-a_2) * np.matmul(d_o,w_o.T))\n",
        "    d_1 = (a_1*(1-a_1) * np.matmul(d_2,w_2.T))\n",
        "\n",
        "\n",
        "    # d_2 = (w_o.dot((d_o.T))).T * (a_2 * 1-a_2)\n",
        " \n",
        "    # gradients\n",
        "    w_o_adj = np.matmul(a_2.T, d_o)\n",
        "    w_2_adj = np.matmul(a_1.T, d_2)\n",
        "    w_1_adj = np.matmul(X.T, d_1)\n",
        "    \n",
        "     \n",
        "    # Updating parameters\n",
        "    w_1 = w_1-(alpha*(w_1_adj))\n",
        "    w_2 = w_2-(alpha*(w_2_adj))\n",
        "    w_o = w_o-(alpha*(w_o_adj))\n",
        "     \n",
        "    return(w_1, w_2, w_o)\n",
        "\n",
        "def trainMLP(X, y, w_1, w_2, w_o, alpha = 0.01, epoch = 10):\n",
        "    accArray = []\n",
        "    lossArray = []\n",
        "    for j in range(epoch):\n",
        "        out = forward_pass(X, w_1, w_2, w_o)\n",
        "        loss = (loss_CE(out, y))\n",
        "        w_1, w_2, w_o = back_pass(X, y, w_1, w_2, w_o, alpha)\n",
        "        ypred = predictMLP(X, w_1, w_2, w_o)\n",
        "        ypred = ypred > 0.5\n",
        "        acc = modelAccuracy(ypred,y,0)\n",
        "        accArray.append(acc)\n",
        "        lossArray.append(loss)\n",
        "        # Print loss every 50 iterations\n",
        "        if (j+1) % 50 == 0:\n",
        "            print(\"epochs:\", j + 1, \"========> Loss:\", loss, \"========> Accuracy:\", acc)  \n",
        "    return(accArray, lossArray, w_1, w_2, w_o)\n",
        "\n",
        "def predictMLP(X, w_1, w_2, w_o):\n",
        "    Out = forward_pass(X, w_1, w_2, w_o)\n",
        "    return Out.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "db_YrJdSrBui"
      },
      "outputs": [],
      "source": [
        "# Initialize Weights\n",
        "w_1 = initialize_wt(X.shape[1], 5) # Input  Layer   to Hidden Layer 1 (D, 5)\n",
        "w_2 = initialize_wt(5, 5)          # Hidden Layer 1 to Hidden Layer 2 (5, 5)\n",
        "w_o = initialize_wt(5, 1)          # Hidden Layer 2 to Output Layer   (5, 5)\n",
        "\n",
        "# print(w_1, \"\\n\\n\", w_2, \"\\n\\n\", w_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "n1vPEg1gtQdH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epochs: 50 ========> Loss: 0.5372414383423855 ========> Accuracy: 77.8957088208136\n",
            "epochs: 100 ========> Loss: 0.5178818932125193 ========> Accuracy: 77.8957088208136\n",
            "epochs: 150 ========> Loss: 0.5088352934271309 ========> Accuracy: 77.8957088208136\n",
            "epochs: 200 ========> Loss: 0.5016296666721893 ========> Accuracy: 77.8957088208136\n",
            "epochs: 250 ========> Loss: 0.49401748919566624 ========> Accuracy: 77.8957088208136\n",
            "epochs: 300 ========> Loss: 0.4857800561629159 ========> Accuracy: 77.8957088208136\n",
            "epochs: 350 ========> Loss: 0.47725450748236486 ========> Accuracy: 78.22853142027985\n",
            "epochs: 400 ========> Loss: 0.4688403865322404 ========> Accuracy: 78.59179511116072\n",
            "epochs: 450 ========> Loss: 0.4608709610880287 ========> Accuracy: 78.8525737942791\n",
            "epochs: 500 ========> Loss: 0.45353308167463774 ========> Accuracy: 79.3041166502623\n",
            "epochs: 550 ========> Loss: 0.4469174030056997 ========> Accuracy: 79.56996884861645\n",
            "epochs: 600 ========> Loss: 0.44102061223800404 ========> Accuracy: 79.85205629572506\n",
            "epochs: 650 ========> Loss: 0.435691092120689 ========> Accuracy: 80.0549969051557\n",
            "epochs: 700 ========> Loss: 0.43083460684788166 ========> Accuracy: 80.26301102982212\n",
            "epochs: 750 ========> Loss: 0.42636644674204977 ========> Accuracy: 80.43551054783818\n",
            "epochs: 800 ========> Loss: 0.4222343429925245 ========> Accuracy: 80.63946586031598\n",
            "epochs: 850 ========> Loss: 0.41846575702565625 ========> Accuracy: 80.80892126919056\n",
            "epochs: 900 ========> Loss: 0.41509093807205144 ========> Accuracy: 80.98040608415947\n",
            "epochs: 950 ========> Loss: 0.412069923002371 ========> Accuracy: 81.13971446256252\n",
            "epochs: 1000 ========> Loss: 0.40934795166491333 ========> Accuracy: 81.30308165315421\n",
            "epochs: 1050 ========> Loss: 0.40689862669513704 ========> Accuracy: 81.39237552130369\n",
            "epochs: 1100 ========> Loss: 0.40469928729934707 ========> Accuracy: 81.48572820164179\n",
            "epochs: 1150 ========> Loss: 0.40270868379404284 ========> Accuracy: 81.6267719251961\n",
            "epochs: 1200 ========> Loss: 0.40087985560892064 ========> Accuracy: 81.72418341772281\n",
            "epochs: 1250 ========> Loss: 0.39917724444862773 ========> Accuracy: 81.8500065955698\n",
            "epochs: 1300 ========> Loss: 0.39757870023499126 ========> Accuracy: 81.95959452466236\n",
            "epochs: 1350 ========> Loss: 0.39607329430785165 ========> Accuracy: 82.07324126594352\n",
            "epochs: 1400 ========> Loss: 0.39465676068641253 ========> Accuracy: 82.16152043104586\n",
            "epochs: 1450 ========> Loss: 0.39332567183624506 ========> Accuracy: 82.2081967712149\n",
            "epochs: 1500 ========> Loss: 0.3920749607334192 ========> Accuracy: 82.30966707593022\n",
            "epochs: 1550 ========> Loss: 0.3908990095503695 ========> Accuracy: 82.44462258120161\n",
            "epochs: 1600 ========> Loss: 0.389794096959893 ========> Accuracy: 82.56841635295432\n",
            "epochs: 1650 ========> Loss: 0.388758695288473 ========> Accuracy: 82.6292985357835\n",
            "epochs: 1700 ========> Loss: 0.38779100257741267 ========> Accuracy: 82.6749601729054\n",
            "epochs: 1750 ========> Loss: 0.38688658371799867 ========> Accuracy: 82.75004819839474\n",
            "epochs: 1800 ========> Loss: 0.3860393486875296 ========> Accuracy: 82.80484216294101\n",
            "epochs: 1850 ========> Loss: 0.38524420771242357 ========> Accuracy: 82.8809448914775\n",
            "epochs: 1900 ========> Loss: 0.3844974344091641 ========> Accuracy: 82.92153301336363\n",
            "epochs: 1950 ========> Loss: 0.3837956821914583 ========> Accuracy: 82.98647400838145\n",
            "epochs: 2000 ========> Loss: 0.38313519997464524 ========> Accuracy: 83.02706213026758\n",
            "epochs: 2050 ========> Loss: 0.3825116690798559 ========> Accuracy: 83.05750322168217\n",
            "epochs: 2100 ========> Loss: 0.3819206357971593 ========> Accuracy: 83.10113545270977\n",
            "epochs: 2150 ========> Loss: 0.38135802937694374 ========> Accuracy: 83.11432659232275\n",
            "epochs: 2200 ========> Loss: 0.38082046776432416 ========> Accuracy: 83.1457823867845\n",
            "epochs: 2250 ========> Loss: 0.38030534101431984 ========> Accuracy: 83.16404704163327\n",
            "epochs: 2300 ========> Loss: 0.3798107283985351 ========> Accuracy: 83.22289981836816\n",
            "epochs: 2350 ========> Loss: 0.37933520814165334 ========> Accuracy: 83.24928209759413\n",
            "epochs: 2400 ========> Loss: 0.37887763190780893 ========> Accuracy: 83.27667907986728\n",
            "epochs: 2450 ========> Loss: 0.37843693688855423 ========> Accuracy: 83.31320838956479\n",
            "epochs: 2500 ========> Loss: 0.37801204100350566 ========> Accuracy: 83.34973769926232\n",
            "epochs: 2550 ========> Loss: 0.37760182393311736 ========> Accuracy: 83.38829641505413\n",
            "epochs: 2600 ========> Loss: 0.37720516467855353 ========> Accuracy: 83.42786983389311\n",
            "epochs: 2650 ========> Loss: 0.37682099966438765 ========> Accuracy: 83.46338444054348\n",
            "epochs: 2700 ========> Loss: 0.3764483747141525 ========> Accuracy: 83.49788434414668\n",
            "epochs: 2750 ========> Loss: 0.3760864771364378 ========> Accuracy: 83.52528132641982\n",
            "epochs: 2800 ========> Loss: 0.3757346446706859 ========> Accuracy: 83.54456068431574\n",
            "epochs: 2850 ========> Loss: 0.37539235345937993 ========> Accuracy: 83.58007529096609\n",
            "epochs: 2900 ========> Loss: 0.37505918944975647 ========> Accuracy: 83.59428113362625\n",
            "epochs: 2950 ========> Loss: 0.3747348104957423 ========> Accuracy: 83.6166046006636\n",
            "epochs: 3000 ========> Loss: 0.3744189090294781 ========> Accuracy: 83.65211920731397\n",
            "epochs: 3050 ========> Loss: 0.3741111836406138 ========> Accuracy: 83.67342797130419\n",
            "epochs: 3100 ========> Loss: 0.37381132261203004 ========> Accuracy: 83.68154559568143\n",
            "epochs: 3150 ========> Loss: 0.3735189976753462 ========> Accuracy: 83.69169262615296\n",
            "epochs: 3200 ========> Loss: 0.3732338644946516 ========> Accuracy: 83.70285435967165\n",
            "epochs: 3250 ========> Loss: 0.37295556697996396 ========> Accuracy: 83.73025134194478\n",
            "epochs: 3300 ========> Loss: 0.372683743742758 ========> Accuracy: 83.76678065164229\n",
            "epochs: 3350 ========> Loss: 0.37241803570475945 ========> Accuracy: 83.76373654250084\n",
            "epochs: 3400 ========> Loss: 0.3721580939275706 ========> Accuracy: 83.77794238516098\n",
            "epochs: 3450 ========> Loss: 0.37190358666065065 ========> Accuracy: 83.79214822782113\n",
            "epochs: 3500 ========> Loss: 0.37165420486512774 ========> Accuracy: 83.81751580399997\n",
            "epochs: 3550 ========> Loss: 0.37140966602349235 ========> Accuracy: 83.82360402228288\n",
            "epochs: 3600 ========> Loss: 0.3711697165377621 ========> Accuracy: 83.83070694361295\n",
            "epochs: 3650 ========> Loss: 0.37093413319125546 ========> Accuracy: 83.84592748932025\n",
            "epochs: 3700 ========> Loss: 0.3707027239862332 ========> Accuracy: 83.87230976854624\n",
            "epochs: 3750 ========> Loss: 0.3704753283115278 ========> Accuracy: 83.90478026605514\n",
            "epochs: 3800 ========> Loss: 0.3702518160420589 ========> Accuracy: 83.92608903004536\n",
            "epochs: 3850 ========> Loss: 0.37003208502867296 ========> Accuracy: 83.94029487270551\n",
            "epochs: 3900 ========> Loss: 0.36981605661982503 ========> Accuracy: 83.94739779403557\n",
            "epochs: 3950 ========> Loss: 0.3696036693203803 ========> Accuracy: 83.96769185497864\n",
            "epochs: 4000 ========> Loss: 0.36939487124077 ========> Accuracy: 83.97783888545017\n",
            "epochs: 4050 ========> Loss: 0.3691896123831328 ========> Accuracy: 83.99204472811032\n",
            "epochs: 4100 ========> Loss: 0.3689878378946734 ========> Accuracy: 84.00726527381762\n",
            "epochs: 4150 ========> Loss: 0.3687894831836254 ========> Accuracy: 84.02857403780783\n",
            "epochs: 4200 ========> Loss: 0.36859447135191614 ========> Accuracy: 84.04480928656228\n",
            "epochs: 4250 ========> Loss: 0.36840271290196586 ========> Accuracy: 84.04683869265659\n",
            "epochs: 4300 ========> Loss: 0.36821410724077547 ========> Accuracy: 84.05901512922243\n",
            "epochs: 4350 ========> Loss: 0.3680285452035939 ========> Accuracy: 84.06814745664681\n",
            "epochs: 4400 ========> Loss: 0.36784591171449926 ========> Accuracy: 84.10061795415572\n",
            "epochs: 4450 ========> Loss: 0.36766608786270144 ========> Accuracy: 84.1077208754858\n",
            "epochs: 4500 ========> Loss: 0.3674889521037352 ========> Accuracy: 84.10366206329718\n",
            "epochs: 4550 ========> Loss: 0.3673143808194118 ========> Accuracy: 84.10975028158009\n",
            "epochs: 4600 ========> Loss: 0.36714224878477286 ========> Accuracy: 84.11380909376871\n",
            "epochs: 4650 ========> Loss: 0.36697243000970803 ========> Accuracy: 84.13308845166462\n",
            "epochs: 4700 ========> Loss: 0.36680479906486435 ========> Accuracy: 84.15033840346624\n",
            "epochs: 4750 ========> Loss: 0.3666392326595658 ========> Accuracy: 84.16352954307922\n",
            "epochs: 4800 ========> Loss: 0.36647561111205335 ========> Accuracy: 84.16860305831499\n",
            "epochs: 4850 ========> Loss: 0.366313819433832 ========> Accuracy: 84.18077949488082\n",
            "epochs: 4900 ========> Loss: 0.36615374792111977 ========> Accuracy: 84.18686771316374\n",
            "epochs: 4950 ========> Loss: 0.36599529229793437 ========> Accuracy: 84.19498533754097\n",
            "epochs: 5000 ========> Loss: 0.36583835353785693 ========> Accuracy: 84.20919118020112\n",
            "epochs: 5050 ========> Loss: 0.36568283750575264 ========> Accuracy: 84.22339702286126\n",
            "epochs: 5100 ========> Loss: 0.36552865453073363 ========> Accuracy: 84.24166167771003\n",
            "epochs: 5150 ========> Loss: 0.36537571897445714 ========> Accuracy: 84.26297044170023\n",
            "epochs: 5200 ========> Loss: 0.36522394881431286 ========> Accuracy: 84.27616158131322\n",
            "epochs: 5250 ========> Loss: 0.36507326522911904 ========> Accuracy: 84.29239683006769\n",
            "epochs: 5300 ========> Loss: 0.3649235921571597 ========> Accuracy: 84.3137055940579\n",
            "epochs: 5350 ========> Loss: 0.3647748557898361 ========> Accuracy: 84.33602906109527\n",
            "epochs: 5400 ========> Loss: 0.3646269839659716 ========> Accuracy: 84.33501435804813\n",
            "epochs: 5450 ========> Loss: 0.36447990544179387 ========> Accuracy: 84.34820549766111\n",
            "epochs: 5500 ========> Loss: 0.3643335490315803 ========> Accuracy: 84.3613966372741\n",
            "epochs: 5550 ========> Loss: 0.36418784264366155 ========> Accuracy: 84.36444074641555\n",
            "epochs: 5600 ========> Loss: 0.3640427122709983 ========> Accuracy: 84.38879361954724\n",
            "epochs: 5650 ========> Loss: 0.36389808102861854 ========> Accuracy: 84.40604357134885\n",
            "epochs: 5700 ========> Loss: 0.36375386836339035 ========> Accuracy: 84.41416119572607\n",
            "epochs: 5750 ========> Loss: 0.3636099896116471 ========> Accuracy: 84.42126411705614\n",
            "epochs: 5800 ========> Loss: 0.3634663561729703 ========> Accuracy: 84.41619060182038\n",
            "epochs: 5850 ========> Loss: 0.3633228767146354 ========> Accuracy: 84.42938174143337\n",
            "epochs: 5900 ========> Loss: 0.36317945998192114 ========> Accuracy: 84.43445525666914\n",
            "epochs: 5950 ========> Loss: 0.3630360198542083 ========> Accuracy: 84.45576402065936\n",
            "epochs: 6000 ========> Loss: 0.36289248308875893 ========> Accuracy: 84.45779342675365\n",
            "epochs: 6050 ========> Loss: 0.3627487995849573 ========> Accuracy: 84.46286694198942\n",
            "epochs: 6100 ========> Loss: 0.36260495400248766 ========> Accuracy: 84.48113159683818\n",
            "epochs: 6150 ========> Loss: 0.36246097654723675 ========> Accuracy: 84.49635214254549\n",
            "epochs: 6200 ========> Loss: 0.3623169503995171 ========> Accuracy: 84.51258739129993\n",
            "epochs: 6250 ========> Loss: 0.3621730141107067 ========> Accuracy: 84.53896967052592\n",
            "epochs: 6300 ========> Loss: 0.36202935886004806 ========> Accuracy: 84.53998437357308\n",
            "epochs: 6350 ========> Loss: 0.361886221332095 ========> Accuracy: 84.5481019979503\n",
            "epochs: 6400 ========> Loss: 0.36174387236209504 ========> Accuracy: 84.56027843451614\n",
            "epochs: 6450 ========> Loss: 0.36160260065586347 ========> Accuracy: 84.5785430893649\n",
            "epochs: 6500 ========> Loss: 0.36146269193617786 ========> Accuracy: 84.58260190155352\n",
            "epochs: 6550 ========> Loss: 0.3613244068389474 ========> Accuracy: 84.5937636350722\n",
            "epochs: 6600 ========> Loss: 0.3611879630634099 ========> Accuracy: 84.60999888382665\n",
            "epochs: 6650 ========> Loss: 0.3610535262068895 ========> Accuracy: 84.61710180515672\n",
            "epochs: 6700 ========> Loss: 0.3609212100216099 ========> Accuracy: 84.61913121125103\n",
            "epochs: 6750 ========> Loss: 0.36079108334300547 ========> Accuracy: 84.63232235086402\n",
            "epochs: 6800 ========> Loss: 0.36066317969508255 ========> Accuracy: 84.63638116305263\n",
            "epochs: 6850 ========> Loss: 0.36053750646951643 ========> Accuracy: 84.64652819352416\n",
            "epochs: 6900 ========> Loss: 0.36041405225778445 ========> Accuracy: 84.65971933313716\n",
            "epochs: 6950 ========> Loss: 0.3602927922286975 ========> Accuracy: 84.66479284837293\n",
            "epochs: 7000 ========> Loss: 0.36017369201034616 ========> Accuracy: 84.66073403618431\n",
            "epochs: 7050 ========> Loss: 0.3600567105580184 ========> Accuracy: 84.64957230266562\n",
            "epochs: 7100 ========> Loss: 0.3599418023124156 ========> Accuracy: 84.65971933313716\n",
            "epochs: 7150 ========> Loss: 0.3598289187964159 ========> Accuracy: 84.671895769703\n",
            "epochs: 7200 ========> Loss: 0.3597180097255314 ========> Accuracy: 84.6871163154103\n",
            "epochs: 7250 ========> Loss: 0.3596090236929347 ========> Accuracy: 84.69523393978751\n",
            "epochs: 7300 ========> Loss: 0.3595019084958426 ========> Accuracy: 84.69218983064606\n",
            "epochs: 7350 ========> Loss: 0.35939661117474514 ========> Accuracy: 84.68305750322168\n",
            "epochs: 7400 ========> Loss: 0.359293077837182 ========> Accuracy: 84.69624864283468\n",
            "epochs: 7450 ========> Loss: 0.3591912533376687 ========> Accuracy: 84.72161621901351\n",
            "epochs: 7500 ========> Loss: 0.3590910808874361 ========> Accuracy: 84.73074854643788\n",
            "epochs: 7550 ========> Loss: 0.35899250166954566 ========> Accuracy: 84.74292498300372\n",
            "epochs: 7600 ========> Loss: 0.3588954545317677 ========> Accuracy: 84.75713082566388\n",
            "epochs: 7650 ========> Loss: 0.358799875817722 ========> Accuracy: 84.77539548051263\n",
            "epochs: 7700 ========> Loss: 0.3587056993764739 ========> Accuracy: 84.78452780793701\n",
            "epochs: 7750 ========> Loss: 0.35861285676602833 ========> Accuracy: 84.80279246278577\n",
            "epochs: 7800 ========> Loss: 0.3585212776425026 ========> Accuracy: 84.81598360239876\n",
            "epochs: 7850 ========> Loss: 0.358430890308667 ========> Accuracy: 84.82714533591744\n",
            "epochs: 7900 ========> Loss: 0.35834162238500766 ========> Accuracy: 84.8322188511532\n",
            "epochs: 7950 ========> Loss: 0.3582534015629944 ========> Accuracy: 84.84845409990767\n",
            "epochs: 8000 ========> Loss: 0.3581661564016877 ========> Accuracy: 84.85352761514343\n",
            "epochs: 8050 ========> Loss: 0.3580798171326738 ========> Accuracy: 84.85657172428489\n",
            "epochs: 8100 ========> Loss: 0.35799431644245305 ========> Accuracy: 84.86367464561496\n",
            "epochs: 8150 ========> Loss: 0.3579095902046415 ========> Accuracy: 84.86671875475642\n",
            "epochs: 8200 ========> Loss: 0.35782557813648763 ========> Accuracy: 84.86773345780357\n",
            "epochs: 8250 ========> Loss: 0.35774222435599456 ========> Accuracy: 84.8758510821808\n",
            "epochs: 8300 ========> Loss: 0.3576594778186703 ========> Accuracy: 84.88193930046371\n",
            "epochs: 8350 ========> Loss: 0.35757729261807053 ========> Accuracy: 84.88092459741657\n",
            "epochs: 8400 ========> Loss: 0.3574956281428473 ========> Accuracy: 84.88193930046371\n",
            "epochs: 8450 ========> Loss: 0.357414449094922 ========> Accuracy: 84.88599811265233\n",
            "epochs: 8500 ========> Loss: 0.35733372538708935 ========> Accuracy: 84.88498340960517\n",
            "epochs: 8550 ========> Loss: 0.35725343195078557 ========> Accuracy: 84.89614514312387\n",
            "epochs: 8600 ========> Loss: 0.3571735484920419 ========> Accuracy: 84.89715984617102\n",
            "epochs: 8650 ========> Loss: 0.35709405923242765 ========> Accuracy: 84.90020395531248\n",
            "epochs: 8700 ========> Loss: 0.35701495266072625 ========> Accuracy: 84.89208633093526\n",
            "epochs: 8750 ========> Loss: 0.35693622130204233 ========> Accuracy: 84.89817454921817\n",
            "epochs: 8800 ========> Loss: 0.356857861489064 ========> Accuracy: 84.9042627675011\n",
            "epochs: 8850 ========> Loss: 0.3567798731024075 ========> Accuracy: 84.90121865835964\n",
            "epochs: 8900 ========> Loss: 0.35670225924004034 ========> Accuracy: 84.90223336140679\n",
            "epochs: 8950 ========> Loss: 0.3566250257834747 ========> Accuracy: 84.91339509492548\n",
            "epochs: 9000 ========> Loss: 0.35654818084965817 ========> Accuracy: 84.91339509492548\n",
            "epochs: 9050 ========> Loss: 0.35647173414611577 ========> Accuracy: 84.91339509492548\n",
            "epochs: 9100 ========> Loss: 0.3563956962736135 ========> Accuracy: 84.91339509492548\n",
            "epochs: 9150 ========> Loss: 0.35632007803644306 ========> Accuracy: 84.923542125397\n",
            "epochs: 9200 ========> Loss: 0.3562448898204487 ========> Accuracy: 84.9255715314913\n",
            "epochs: 9250 ========> Loss: 0.3561701410843819 ========> Accuracy: 84.92760093758561\n",
            "epochs: 9300 ========> Loss: 0.35609583998784533 ========> Accuracy: 84.93267445282137\n",
            "epochs: 9350 ========> Loss: 0.3560219931580646 ========> Accuracy: 84.94890970157584\n",
            "epochs: 9400 ========> Loss: 0.35594860558516334 ========> Accuracy: 84.94688029548153\n",
            "epochs: 9450 ========> Loss: 0.355875680633646 ========> Accuracy: 84.94485088938723\n",
            "epochs: 9500 ========> Loss: 0.35580322016277305 ========> Accuracy: 84.94383618634006\n",
            "epochs: 9550 ========> Loss: 0.35573122475306107 ========> Accuracy: 84.94079207719861\n",
            "epochs: 9600 ========> Loss: 0.35565969403299097 ========> Accuracy: 84.9387626711043\n",
            "epochs: 9650 ========> Loss: 0.3555886270857544 ========> Accuracy: 84.95093910767014\n",
            "epochs: 9700 ========> Loss: 0.3555180228931633 ========> Accuracy: 84.95499791985876\n",
            "epochs: 9750 ========> Loss: 0.355447880750717 ========> Accuracy: 84.95296851376445\n",
            "epochs: 9800 ========> Loss: 0.35537820057456326 ========> Accuracy: 84.96108613814167\n",
            "epochs: 9850 ========> Loss: 0.3553089830259816 ========> Accuracy: 84.96311554423598\n",
            "epochs: 9900 ========> Loss: 0.35524022940464356 ========> Accuracy: 84.96514495033028\n",
            "epochs: 9950 ========> Loss: 0.35517194130363 ========> Accuracy: 84.9712331686132\n",
            "epochs: 10000 ========> Loss: 0.35510412006641484 ========> Accuracy: 84.9692037625189\n",
            "epochs: 10050 ========> Loss: 0.3550367661260605 ========> Accuracy: 84.97833608994327\n",
            "epochs: 10100 ========> Loss: 0.35496987832963994 ========> Accuracy: 84.97732138689612\n",
            "epochs: 10150 ========> Loss: 0.35490345335263745 ========> Accuracy: 84.97427727775467\n",
            "epochs: 10200 ========> Loss: 0.3548374852919504 ========> Accuracy: 84.97935079299043\n",
            "epochs: 10250 ========> Loss: 0.35477196549982165 ========> Accuracy: 84.98340960517903\n",
            "epochs: 10300 ========> Loss: 0.3547068826927251 ========> Accuracy: 84.97732138689612\n",
            "epochs: 10350 ========> Loss: 0.354642223344119 ========> Accuracy: 84.98340960517903\n",
            "epochs: 10400 ========> Loss: 0.35457797234895044 ========> Accuracy: 84.98848312041481\n",
            "epochs: 10450 ========> Loss: 0.354514113928304 ========> Accuracy: 84.98949782346197\n",
            "epochs: 10500 ========> Loss: 0.3544506327208237 ========> Accuracy: 84.9864537143205\n",
            "epochs: 10550 ========> Loss: 0.35438751498094523 ========> Accuracy: 84.99355663565058\n",
            "epochs: 10600 ========> Loss: 0.35432474977342127 ========> Accuracy: 85.00065955698065\n",
            "epochs: 10650 ========> Loss: 0.35426233002498786 ========> Accuracy: 84.99660074479203\n",
            "epochs: 10700 ========> Loss: 0.35420025327826615 ========> Accuracy: 84.99660074479203\n",
            "epochs: 10750 ========> Loss: 0.3541385220038527 ========> Accuracy: 84.99863015088634\n",
            "epochs: 10800 ========> Loss: 0.3540771433744836 ========> Accuracy: 85.00979188440503\n",
            "epochs: 10850 ========> Loss: 0.35401612848952985 ========> Accuracy: 85.00776247831072\n",
            "epochs: 10900 ========> Loss: 0.3539554911418661 ========> Accuracy: 85.00268896307496\n",
            "epochs: 10950 ========> Loss: 0.35389524631229907 ========> Accuracy: 85.00877718135787\n",
            "epochs: 11000 ========> Loss: 0.35383540862916274 ========> Accuracy: 85.01588010268794\n",
            "epochs: 11050 ========> Loss: 0.3537759910277992 ========> Accuracy: 85.02805653925378\n",
            "epochs: 11100 ========> Loss: 0.3537170037942 ========> Accuracy: 85.01588010268794\n",
            "epochs: 11150 ========> Loss: 0.35365845410130775 ========> Accuracy: 85.01283599354649\n",
            "epochs: 11200 ========> Loss: 0.35360034606470303 ========> Accuracy: 85.02095361792371\n",
            "epochs: 11250 ========> Loss: 0.3535426812587732 ========> Accuracy: 85.02095361792371\n",
            "epochs: 11300 ========> Loss: 0.35348545953192084 ========> Accuracy: 85.02399772706517\n",
            "epochs: 11350 ========> Loss: 0.3534286798379861 ========> Accuracy: 85.02399772706517\n",
            "epochs: 11400 ========> Loss: 0.35337234071192547 ========> Accuracy: 85.01588010268794\n",
            "epochs: 11450 ========> Loss: 0.353316440074614 ========> Accuracy: 85.02095361792371\n",
            "epochs: 11500 ========> Loss: 0.3532609743374294 ========> Accuracy: 85.02602713315947\n",
            "epochs: 11550 ========> Loss: 0.35320593718400295 ========> Accuracy: 85.02196832097087\n",
            "epochs: 11600 ========> Loss: 0.3531513186345441 ========> Accuracy: 85.03313005448956\n",
            "epochs: 11650 ========> Loss: 0.3530971048422127 ========> Accuracy: 85.03921827277247\n",
            "epochs: 11700 ========> Loss: 0.3530432786675112 ========> Accuracy: 85.04226238191393\n",
            "epochs: 11750 ========> Loss: 0.3529898207479599 ========> Accuracy: 85.04327708496109\n",
            "epochs: 11800 ========> Loss: 0.3529367106983537 ========> Accuracy: 85.05443881847977\n",
            "epochs: 11850 ========> Loss: 0.3528839281780879 ========> Accuracy: 85.06052703676269\n",
            "epochs: 11900 ========> Loss: 0.3528314537053347 ========> Accuracy: 85.06357114590415\n",
            "epochs: 11950 ========> Loss: 0.35277926919981745 ========> Accuracy: 85.05748292762124\n",
            "epochs: 12000 ========> Loss: 0.3527273582845237 ========> Accuracy: 85.0645858489513\n",
            "epochs: 12050 ========> Loss: 0.3526757063910562 ========> Accuracy: 85.06864466113991\n",
            "epochs: 12100 ========> Loss: 0.35262430071327977 ========> Accuracy: 85.07371817637568\n",
            "epochs: 12150 ========> Loss: 0.3525731300505462 ========> Accuracy: 85.07067406723422\n",
            "epochs: 12200 ========> Loss: 0.3525221845787548 ========> Accuracy: 85.07473287942283\n",
            "epochs: 12250 ========> Loss: 0.35247145558471543 ========> Accuracy: 85.07574758246999\n",
            "epochs: 12300 ========> Loss: 0.3524209351956195 ========> Accuracy: 85.07676228551713\n",
            "epochs: 12350 ========> Loss: 0.3523706161301988 ========> Accuracy: 85.07067406723422\n",
            "epochs: 12400 ========> Loss: 0.35232049149160793 ========> Accuracy: 85.06864466113991\n",
            "epochs: 12450 ========> Loss: 0.35227055461521456 ========> Accuracy: 85.08082109770577\n",
            "epochs: 12500 ========> Loss: 0.3522207989785474 ========> Accuracy: 85.08183580075291\n",
            "epochs: 12550 ========> Loss: 0.35217121817657354 ========> Accuracy: 85.0798063946586\n",
            "epochs: 12600 ========> Loss: 0.35212180596341447 ========> Accuracy: 85.08589461294153\n",
            "epochs: 12650 ========> Loss: 0.3520725563608357 ========> Accuracy: 85.0929975342716\n",
            "epochs: 12700 ========> Loss: 0.35202346383279576 ========> Accuracy: 85.10415926779028\n",
            "epochs: 12750 ========> Loss: 0.35197452352191216 ========> Accuracy: 85.09807104950737\n",
            "epochs: 12800 ========> Loss: 0.3519257315358101 ========> Accuracy: 85.11024748607319\n",
            "epochs: 12850 ========> Loss: 0.35187708525768124 ========> Accuracy: 85.11836511045043\n",
            "epochs: 12900 ========> Loss: 0.3518285836367661 ========> Accuracy: 85.12242392263903\n",
            "epochs: 12950 ========> Loss: 0.351780227394973 ========> Accuracy: 85.1254680317805\n",
            "epochs: 13000 ========> Loss: 0.3517320190735068 ========> Accuracy: 85.12648273482766\n",
            "epochs: 13050 ========> Loss: 0.3516839628485601 ========> Accuracy: 85.12445332873335\n",
            "epochs: 13100 ========> Loss: 0.35163606407588305 ========> Accuracy: 85.11633570435612\n",
            "epochs: 13150 ========> Loss: 0.3515883285793602 ========> Accuracy: 85.12039451654474\n",
            "epochs: 13200 ========> Loss: 0.3515407617642501 ========> Accuracy: 85.11532100130896\n",
            "epochs: 13250 ========> Loss: 0.3514933676872653 ========> Accuracy: 85.11735040740326\n",
            "epochs: 13300 ========> Loss: 0.3514461482310176 ========> Accuracy: 85.11532100130896\n",
            "epochs: 13350 ========> Loss: 0.3513991025025875 ========> Accuracy: 85.11836511045043\n",
            "epochs: 13400 ========> Loss: 0.3513522265177301 ========> Accuracy: 85.11532100130896\n",
            "epochs: 13450 ========> Loss: 0.35130551316780484 ========> Accuracy: 85.11633570435612\n",
            "epochs: 13500 ========> Loss: 0.3512589524182483 ========> Accuracy: 85.11937981349757\n",
            "epochs: 13550 ========> Loss: 0.35121253166652194 ========> Accuracy: 85.11633570435612\n",
            "epochs: 13600 ========> Loss: 0.35116623619346565 ========> Accuracy: 85.11836511045043\n",
            "epochs: 13650 ========> Loss: 0.35112004966788146 ========> Accuracy: 85.12749743787481\n",
            "epochs: 13700 ========> Loss: 0.35107395470173963 ========> Accuracy: 85.13764446834634\n",
            "epochs: 13750 ========> Loss: 0.35102793349543837 ========> Accuracy: 85.13460035920488\n",
            "epochs: 13800 ========> Loss: 0.35098196865075065 ========> Accuracy: 85.13561506225203\n",
            "epochs: 13850 ========> Loss: 0.3509360442478465 ========> Accuracy: 85.12648273482766\n",
            "epochs: 13900 ========> Loss: 0.35089014725078754 ========> Accuracy: 85.13561506225203\n",
            "epochs: 13950 ========> Loss: 0.3508442691756204 ========> Accuracy: 85.13054154701626\n",
            "epochs: 14000 ========> Loss: 0.35079840769500487 ========> Accuracy: 85.13764446834634\n",
            "epochs: 14050 ========> Loss: 0.3507525675347357 ========> Accuracy: 85.14170328053495\n",
            "epochs: 14100 ========> Loss: 0.3507067599006046 ========> Accuracy: 85.14880620186503\n",
            "epochs: 14150 ========> Loss: 0.35066100009017664 ========> Accuracy: 85.14576209272356\n",
            "epochs: 14200 ========> Loss: 0.3506153038689887 ========> Accuracy: 85.1579385292894\n",
            "epochs: 14250 ========> Loss: 0.350569683973503 ========> Accuracy: 85.16098263843087\n",
            "epochs: 14300 ========> Loss: 0.3505241480170228 ========> Accuracy: 85.16301204452516\n",
            "epochs: 14350 ========> Loss: 0.35047869819550465 ========> Accuracy: 85.1711296689024\n",
            "epochs: 14400 ========> Loss: 0.3504333323075894 ========> Accuracy: 85.17011496585523\n",
            "epochs: 14450 ========> Loss: 0.3503880452927562 ========> Accuracy: 85.17721788718532\n",
            "epochs: 14500 ========> Loss: 0.35034283067997635 ========> Accuracy: 85.17924729327963\n",
            "epochs: 14550 ========> Loss: 0.35029768166630026 ========> Accuracy: 85.1731590749967\n",
            "epochs: 14600 ========> Loss: 0.35025259177675694 ========> Accuracy: 85.17620318413816\n",
            "epochs: 14650 ========> Loss: 0.35020755515725427 ========> Accuracy: 85.17823259023247\n",
            "epochs: 14700 ========> Loss: 0.3501625665730115 ========> Accuracy: 85.18127669937392\n",
            "epochs: 14750 ========> Loss: 0.3501176211792332 ========> Accuracy: 85.1711296689024\n",
            "epochs: 14800 ========> Loss: 0.3500727141252548 ========> Accuracy: 85.17518848109101\n",
            "epochs: 14850 ========> Loss: 0.35002784005446563 ========> Accuracy: 85.17721788718532\n",
            "epochs: 14900 ========> Loss: 0.3499829925662481 ========> Accuracy: 85.17823259023247\n",
            "epochs: 14950 ========> Loss: 0.3499381637071994 ========> Accuracy: 85.17620318413816\n",
            "epochs: 15000 ========> Loss: 0.3498933435531836 ========> Accuracy: 85.18127669937392\n",
            "epochs: 15050 ========> Loss: 0.34984851993069827 ========> Accuracy: 85.17721788718532\n",
            "epochs: 15100 ========> Loss: 0.34980367830788556 ========> Accuracy: 85.17518848109101\n",
            "epochs: 15150 ========> Loss: 0.3497588018660615 ========> Accuracy: 85.17417377804387\n",
            "epochs: 15200 ========> Loss: 0.3497138717456455 ========> Accuracy: 85.17214437194956\n",
            "epochs: 15250 ========> Loss: 0.3496688674482655 ========> Accuracy: 85.16910026280809\n",
            "epochs: 15300 ========> Loss: 0.34962376737014206 ========> Accuracy: 85.16504145061947\n",
            "epochs: 15350 ========> Loss: 0.3495785494394096 ========> Accuracy: 85.16098263843087\n",
            "epochs: 15400 ========> Loss: 0.3495331918297418 ========> Accuracy: 85.15489442014794\n",
            "epochs: 15450 ========> Loss: 0.3494876737224671 ========> Accuracy: 85.16199734147801\n",
            "epochs: 15500 ========> Loss: 0.34944197608815813 ========> Accuracy: 85.16301204452516\n",
            "epochs: 15550 ========> Loss: 0.3493960824564632 ========> Accuracy: 85.16098263843087\n",
            "epochs: 15600 ========> Loss: 0.34934997964073267 ========> Accuracy: 85.16808555976093\n",
            "epochs: 15650 ========> Loss: 0.34930365838325456 ========> Accuracy: 85.16808555976093\n",
            "epochs: 15700 ========> Loss: 0.3492571138888977 ========> Accuracy: 85.17924729327963\n",
            "epochs: 15750 ========> Loss: 0.3492103462202783 ========> Accuracy: 85.17620318413816\n",
            "epochs: 15800 ========> Loss: 0.34916336053582153 ========> Accuracy: 85.17823259023247\n",
            "epochs: 15850 ========> Loss: 0.34911616716212757 ========> Accuracy: 85.17924729327963\n",
            "epochs: 15900 ========> Loss: 0.3490687815022778 ========> Accuracy: 85.18127669937392\n",
            "epochs: 15950 ========> Loss: 0.3490212237906768 ========> Accuracy: 85.188379620704\n",
            "epochs: 16000 ========> Loss: 0.3489735187118977 ========> Accuracy: 85.21171779078853\n",
            "epochs: 16050 ========> Loss: 0.34892569490571984 ========> Accuracy: 85.21577660297713\n",
            "epochs: 16100 ========> Loss: 0.34887778438366834 ========> Accuracy: 85.21679130602429\n",
            "epochs: 16150 ========> Loss: 0.34882982188457734 ========> Accuracy: 85.21780600907145\n",
            "epochs: 16200 ========> Loss: 0.3487818441983196 ========> Accuracy: 85.2188207121186\n",
            "epochs: 16250 ========> Loss: 0.34873388948756895 ========> Accuracy: 85.21679130602429\n",
            "epochs: 16300 ========> Loss: 0.3486859966365057 ========> Accuracy: 85.21577660297713\n",
            "epochs: 16350 ========> Loss: 0.34863820465188733 ========> Accuracy: 85.20867368164706\n",
            "epochs: 16400 ========> Loss: 0.3485905521355166 ========> Accuracy: 85.21273249383567\n",
            "epochs: 16450 ========> Loss: 0.3485430768382254 ========> Accuracy: 85.21679130602429\n",
            "epochs: 16500 ========> Loss: 0.3484958152952057 ========> Accuracy: 85.22795303954298\n",
            "epochs: 16550 ========> Loss: 0.3484488025325076 ========> Accuracy: 85.2320118517316\n",
            "epochs: 16600 ========> Loss: 0.34840207182654837 ========> Accuracy: 85.22896774259013\n",
            "epochs: 16650 ========> Loss: 0.34835565449407363 ========> Accuracy: 85.22287952430722\n",
            "epochs: 16700 ========> Loss: 0.34830957969007154 ========> Accuracy: 85.22186482126006\n",
            "epochs: 16750 ========> Loss: 0.3482638741957404 ========> Accuracy: 85.2188207121186\n",
            "epochs: 16800 ========> Loss: 0.34821856218687064 ========> Accuracy: 85.23302655477875\n",
            "epochs: 16850 ========> Loss: 0.348173664983288 ========> Accuracy: 85.24621769439173\n",
            "epochs: 16900 ========> Loss: 0.34812920079018816 ========> Accuracy: 85.24723239743889\n",
            "epochs: 16950 ========> Loss: 0.34808518445023773 ========> Accuracy: 85.25940883400473\n",
            "epochs: 17000 ========> Loss: 0.34804162722965687 ========> Accuracy: 85.25535002181611\n",
            "epochs: 17050 ========> Loss: 0.34799853666148356 ========> Accuracy: 85.25027650658035\n",
            "epochs: 17100 ========> Loss: 0.3479559164651393 ========> Accuracy: 85.26042353705188\n",
            "epochs: 17150 ========> Loss: 0.3479137665544026 ========> Accuracy: 85.25940883400473\n",
            "epochs: 17200 ========> Loss: 0.3478720831374984 ========> Accuracy: 85.25535002181611\n",
            "epochs: 17250 ========> Loss: 0.34783085890485804 ========> Accuracy: 85.25535002181611\n",
            "epochs: 17300 ========> Loss: 0.3477900832934561 ========> Accuracy: 85.25636472486326\n",
            "epochs: 17350 ========> Loss: 0.34774974281228027 ========> Accuracy: 85.25940883400473\n",
            "epochs: 17400 ========> Loss: 0.3477098214116632 ========> Accuracy: 85.26346764619333\n",
            "epochs: 17450 ========> Loss: 0.347670300879678 ========> Accuracy: 85.26346764619333\n",
            "epochs: 17500 ========> Loss: 0.3476311612510683 ========> Accuracy: 85.26143824009903\n",
            "epochs: 17550 ========> Loss: 0.34759238121759395 ========> Accuracy: 85.25940883400473\n",
            "epochs: 17600 ========> Loss: 0.3475539385326003 ========> Accuracy: 85.26346764619333\n",
            "epochs: 17650 ========> Loss: 0.34751581040653307 ========> Accuracy: 85.25940883400473\n",
            "epochs: 17700 ========> Loss: 0.3474779738936208 ========> Accuracy: 85.2644823492405\n",
            "epochs: 17750 ========> Loss: 0.34744040627281986 ========> Accuracy: 85.2533206157218\n",
            "epochs: 17800 ========> Loss: 0.3474030854282418 ========> Accuracy: 85.24824710048604\n",
            "epochs: 17850 ========> Loss: 0.3473659902356532 ========> Accuracy: 85.25230591267466\n",
            "epochs: 17900 ========> Loss: 0.34732910096226527 ========> Accuracy: 85.25027650658035\n",
            "epochs: 17950 ========> Loss: 0.3472923996868483 ========> Accuracy: 85.25535002181611\n",
            "epochs: 18000 ========> Loss: 0.3472558707459502 ========> Accuracy: 85.26042353705188\n",
            "epochs: 18050 ========> Loss: 0.3472195012090363 ========> Accuracy: 85.2512912096275\n",
            "epochs: 18100 ========> Loss: 0.34718328137958565 ========> Accuracy: 85.24723239743889\n",
            "epochs: 18150 ========> Loss: 0.3471472053090689 ========> Accuracy: 85.24723239743889\n",
            "epochs: 18200 ========> Loss: 0.34711127129473063 ========> Accuracy: 85.24621769439173\n",
            "epochs: 18250 ========> Loss: 0.3470754823095497 ========> Accuracy: 85.24317358525028\n",
            "epochs: 18300 ========> Loss: 0.34703984628521306 ========> Accuracy: 85.23505596087305\n",
            "epochs: 18350 ========> Loss: 0.3470043761417552 ========> Accuracy: 85.23302655477875\n",
            "epochs: 18400 ========> Loss: 0.3469690894407806 ========> Accuracy: 85.2340412578259\n",
            "epochs: 18450 ========> Loss: 0.34693400754676806 ========> Accuracy: 85.24317358525028\n",
            "epochs: 18500 ========> Loss: 0.3468991542266363 ========> Accuracy: 85.23505596087305\n",
            "epochs: 18550 ========> Loss: 0.3468645537077334 ========> Accuracy: 85.2360706639202\n",
            "epochs: 18600 ========> Loss: 0.34683022833899746 ========> Accuracy: 85.24317358525028\n",
            "epochs: 18650 ========> Loss: 0.3467961961287793 ========> Accuracy: 85.24723239743889\n",
            "epochs: 18700 ========> Loss: 0.346762468520186 ========> Accuracy: 85.2512912096275\n",
            "epochs: 18750 ========> Loss: 0.34672904876757077 ========> Accuracy: 85.25433531876897\n",
            "epochs: 18800 ========> Loss: 0.34669593117862685 ========> Accuracy: 85.25535002181611\n",
            "epochs: 18850 ========> Loss: 0.3466631013086675 ========> Accuracy: 85.25737942791042\n",
            "epochs: 18900 ========> Loss: 0.3466305369949956 ========> Accuracy: 85.2665117553348\n",
            "epochs: 18950 ========> Loss: 0.34659820996603746 ========> Accuracy: 85.27564408275919\n",
            "epochs: 19000 ========> Loss: 0.34656608769569397 ========> Accuracy: 85.27462937971202\n",
            "epochs: 19050 ========> Loss: 0.34653413520230825 ========> Accuracy: 85.28071759799495\n",
            "epochs: 19100 ========> Loss: 0.34650231658398073 ========> Accuracy: 85.27970289494779\n",
            "epochs: 19150 ========> Loss: 0.3464705961943131 ========> Accuracy: 85.27970289494779\n",
            "epochs: 19200 ========> Loss: 0.34643893945868015 ========> Accuracy: 85.27361467666488\n",
            "epochs: 19250 ========> Loss: 0.34640731339217234 ========> Accuracy: 85.27970289494779\n",
            "epochs: 19300 ========> Loss: 0.34637568690498755 ========> Accuracy: 85.28680581627786\n",
            "epochs: 19350 ========> Loss: 0.346344030978316 ========> Accuracy: 85.2837617071364\n",
            "epochs: 19400 ========> Loss: 0.34631231877575697 ========> Accuracy: 85.29390873760794\n",
            "epochs: 19450 ========> Loss: 0.3462805257323833 ========> Accuracy: 85.28782051932502\n",
            "epochs: 19500 ========> Loss: 0.34624862964245884 ========> Accuracy: 85.27868819190064\n",
            "epochs: 19550 ========> Loss: 0.34621661075078125 ========> Accuracy: 85.28579111323072\n",
            "epochs: 19600 ========> Loss: 0.34618445184248897 ========> Accuracy: 85.2949234406551\n",
            "epochs: 19650 ========> Loss: 0.34615213832132824 ========> Accuracy: 85.29999695589086\n",
            "epochs: 19700 ========> Loss: 0.34611965826562224 ========> Accuracy: 85.29999695589086\n",
            "epochs: 19750 ========> Loss: 0.3460870024531618 ========> Accuracy: 85.30507047112663\n",
            "epochs: 19800 ========> Loss: 0.34605416434970077 ========> Accuracy: 85.31623220464532\n",
            "epochs: 19850 ========> Loss: 0.3460211400597069 ========> Accuracy: 85.32029101683392\n",
            "epochs: 19900 ========> Loss: 0.345987928241789 ========> Accuracy: 85.31724690769246\n",
            "epochs: 19950 ========> Loss: 0.34595452999449217 ========> Accuracy: 85.31420279855101\n",
            "epochs: 20000 ========> Loss: 0.34592094872102924 ========> Accuracy: 85.30912928331523\n",
            "epochs: 20050 ========> Loss: 0.34588718998447493 ========> Accuracy: 85.31014398636239\n",
            "epochs: 20100 ========> Loss: 0.34585326136863237 ========> Accuracy: 85.31115868940955\n",
            "epochs: 20150 ========> Loss: 0.34581917236444953 ========> Accuracy: 85.31623220464532\n",
            "epochs: 20200 ========> Loss: 0.34578493430674606 ========> Accuracy: 85.32232042292823\n",
            "epochs: 20250 ========> Loss: 0.3457505603887415 ========> Accuracy: 85.31927631378677\n",
            "epochs: 20300 ========> Loss: 0.3457160657785406 ========> Accuracy: 85.31724690769246\n",
            "epochs: 20350 ========> Loss: 0.34568146784802856 ========> Accuracy: 85.31014398636239\n",
            "epochs: 20400 ========> Loss: 0.3456467864979094 ========> Accuracy: 85.31318809550386\n",
            "epochs: 20450 ========> Loss: 0.3456120445244168 ========> Accuracy: 85.3121733924567\n",
            "epochs: 20500 ========> Loss: 0.34557726793097804 ========> Accuracy: 85.31623220464532\n",
            "epochs: 20550 ========> Loss: 0.34554248605486887 ========> Accuracy: 85.32130571988108\n",
            "epochs: 20600 ========> Loss: 0.3455077313704815 ========> Accuracy: 85.32840864121115\n",
            "epochs: 20650 ========> Loss: 0.34547303886018416 ========> Accuracy: 85.33246745339976\n",
            "epochs: 20700 ========> Loss: 0.3454384449142478 ========> Accuracy: 85.33348215644692\n",
            "epochs: 20750 ========> Loss: 0.3454039858215283 ========> Accuracy: 85.33348215644692\n",
            "epochs: 20800 ========> Loss: 0.3453696960163508 ========> Accuracy: 85.33145275035261\n",
            "epochs: 20850 ========> Loss: 0.3453356063209696 ========> Accuracy: 85.33551156254123\n",
            "epochs: 20900 ========> Loss: 0.34530174243988165 ========> Accuracy: 85.34058507777699\n",
            "epochs: 20950 ========> Loss: 0.34526812391414313 ========> Accuracy: 85.33043804730546\n",
            "epochs: 21000 ========> Loss: 0.3452347636466681 ========> Accuracy: 85.32637923511685\n",
            "epochs: 21050 ========> Loss: 0.3452016679960841 ========> Accuracy: 85.32434982902254\n",
            "epochs: 21100 ========> Loss: 0.3451688373417124 ========> Accuracy: 85.32029101683392\n",
            "epochs: 21150 ========> Loss: 0.3451362669678778 ========> Accuracy: 85.3121733924567\n",
            "epochs: 21200 ========> Loss: 0.34510394810605854 ========> Accuracy: 85.31826161073963\n",
            "epochs: 21250 ========> Loss: 0.3450718689976375 ========> Accuracy: 85.31623220464532\n",
            "epochs: 21300 ========> Loss: 0.3450400158812086 ========> Accuracy: 85.30912928331523\n",
            "epochs: 21350 ========> Loss: 0.34500837385104793 ========> Accuracy: 85.3121733924567\n",
            "epochs: 21400 ========> Loss: 0.34497692756774445 ========> Accuracy: 85.31115868940955\n",
            "epochs: 21450 ========> Loss: 0.3449456618244613 ========> Accuracy: 85.31318809550386\n",
            "epochs: 21500 ========> Loss: 0.34491456198360265 ========> Accuracy: 85.31014398636239\n",
            "epochs: 21550 ========> Loss: 0.34488361430175085 ========> Accuracy: 85.3121733924567\n",
            "epochs: 21600 ========> Loss: 0.34485280615903324 ========> Accuracy: 85.30811458026808\n",
            "epochs: 21650 ========> Loss: 0.3448221262055444 ========> Accuracy: 85.30811458026808\n",
            "epochs: 21700 ========> Loss: 0.3447915644341609 ========> Accuracy: 85.31318809550386\n",
            "epochs: 21750 ========> Loss: 0.3447611121871999 ========> Accuracy: 85.31014398636239\n",
            "epochs: 21800 ========> Loss: 0.34473076210421033 ========> Accuracy: 85.30608517417377\n",
            "epochs: 21850 ========> Loss: 0.3447005080194059 ========> Accuracy: 85.31927631378677\n",
            "epochs: 21900 ========> Loss: 0.344670344819113 ========> Accuracy: 85.31927631378677\n",
            "epochs: 21950 ========> Loss: 0.3446402682711882 ========> Accuracy: 85.327393938164\n",
            "epochs: 22000 ========> Loss: 0.3446102748388911 ========> Accuracy: 85.32637923511685\n",
            "epochs: 22050 ========> Loss: 0.3445803614907313 ========> Accuracy: 85.327393938164\n",
            "epochs: 22100 ========> Loss: 0.34455052551539594 ========> Accuracy: 85.33246745339976\n",
            "epochs: 22150 ========> Loss: 0.34452076434749346 ========> Accuracy: 85.33855567168268\n",
            "epochs: 22200 ========> Loss: 0.3444910754064055 ========> Accuracy: 85.33551156254123\n",
            "epochs: 22250 ========> Loss: 0.3444614559480197 ========> Accuracy: 85.33246745339976\n",
            "epochs: 22300 ========> Loss: 0.34443190292837056 ========> Accuracy: 85.33348215644692\n",
            "epochs: 22350 ========> Loss: 0.3444024128796021 ========> Accuracy: 85.34261448387129\n",
            "epochs: 22400 ========> Loss: 0.34437298180173137 ========> Accuracy: 85.34971740520136\n",
            "epochs: 22450 ========> Loss: 0.34434360507724554 ========> Accuracy: 85.3466732960599\n",
            "epochs: 22500 ========> Loss: 0.3443142774180657 ========> Accuracy: 85.33754096863552\n",
            "epochs: 22550 ========> Loss: 0.34428499285475095 ========> Accuracy: 85.32840864121115\n",
            "epochs: 22600 ========> Loss: 0.3442557447759223 ========> Accuracy: 85.33449685949407\n",
            "epochs: 22650 ========> Loss: 0.34422652602269815 ========> Accuracy: 85.3294233442583\n",
            "epochs: 22700 ========> Loss: 0.3441973290396859 ========> Accuracy: 85.33754096863552\n",
            "epochs: 22750 ========> Loss: 0.34416814608145213 ========> Accuracy: 85.33754096863552\n",
            "epochs: 22800 ========> Loss: 0.3441389694710755 ========> Accuracy: 85.33652626558838\n",
            "epochs: 22850 ========> Loss: 0.3441097919044216 ========> Accuracy: 85.34261448387129\n",
            "epochs: 22900 ========> Loss: 0.34408060678934205 ========> Accuracy: 85.34362918691845\n",
            "epochs: 22950 ========> Loss: 0.34405140860309613 ========> Accuracy: 85.3446438899656\n",
            "epochs: 23000 ========> Loss: 0.34402219324493505 ========> Accuracy: 85.35073210824852\n",
            "epochs: 23050 ========> Loss: 0.3439929583556631 ========> Accuracy: 85.3446438899656\n",
            "epochs: 23100 ========> Loss: 0.343963703573838 ========> Accuracy: 85.34870270215421\n",
            "epochs: 23150 ========> Loss: 0.3439344307002399 ========> Accuracy: 85.34565859301276\n",
            "epochs: 23200 ========> Loss: 0.3439051437485503 ========> Accuracy: 85.34565859301276\n",
            "epochs: 23250 ========> Loss: 0.3438758488700156 ========> Accuracy: 85.34971740520136\n",
            "epochs: 23300 ========> Loss: 0.34384655415166904 ========> Accuracy: 85.34870270215421\n",
            "epochs: 23350 ========> Loss: 0.3438172692995264 ========> Accuracy: 85.35682032653143\n",
            "epochs: 23400 ========> Loss: 0.34378800522826325 ========> Accuracy: 85.35884973262576\n",
            "epochs: 23450 ========> Loss: 0.3437587735859738 ========> Accuracy: 85.36290854481436\n",
            "epochs: 23500 ========> Loss: 0.34372958624615 ========> Accuracy: 85.36189384176721\n",
            "epochs: 23550 ========> Loss: 0.34370045479918404 ========> Accuracy: 85.36087913872007\n",
            "epochs: 23600 ========> Loss: 0.34367139007314257 ========> Accuracy: 85.36189384176721\n",
            "epochs: 23650 ========> Loss: 0.34364240170919463 ========> Accuracy: 85.36189384176721\n",
            "epochs: 23700 ========> Loss: 0.3436134978117146 ========> Accuracy: 85.36595265395583\n",
            "epochs: 23750 ========> Loss: 0.3435846846873731 ========> Accuracy: 85.36798206005012\n",
            "epochs: 23800 ========> Loss: 0.3435559666818478 ========> Accuracy: 85.3710261691916\n",
            "epochs: 23850 ========> Loss: 0.3435273461173805 ========> Accuracy: 85.37204087223874\n",
            "epochs: 23900 ========> Loss: 0.3434988233294015 ========> Accuracy: 85.37305557528589\n",
            "epochs: 23950 ========> Loss: 0.3434703967959716 ========> Accuracy: 85.3771143874745\n",
            "epochs: 24000 ========> Loss: 0.3434420633499593 ========> Accuracy: 85.37914379356882\n",
            "epochs: 24050 ========> Loss: 0.343413818460743 ========> Accuracy: 85.37609968442736\n",
            "epochs: 24100 ========> Loss: 0.3433856565698425 ========> Accuracy: 85.38218790271027\n",
            "epochs: 24150 ========> Loss: 0.34335757146323903 ========> Accuracy: 85.39537904232326\n",
            "epochs: 24200 ========> Loss: 0.3433295566622054 ========> Accuracy: 85.40045255755902\n",
            "epochs: 24250 ========> Loss: 0.3433016058141852 ========> Accuracy: 85.40146726060618\n",
            "epochs: 24300 ========> Loss: 0.34327371306552956 ========> Accuracy: 85.39740844841756\n",
            "epochs: 24350 ========> Loss: 0.34324587339859625 ========> Accuracy: 85.40045255755902\n",
            "epochs: 24400 ========> Loss: 0.3432180829167593 ========> Accuracy: 85.40146726060618\n",
            "epochs: 24450 ========> Loss: 0.3431903390623331 ========> Accuracy: 85.40045255755902\n",
            "epochs: 24500 ========> Loss: 0.3431626407545278 ========> Accuracy: 85.40349666670049\n",
            "epochs: 24550 ========> Loss: 0.34313498843770407 ========> Accuracy: 85.40146726060618\n",
            "epochs: 24600 ========> Loss: 0.34310738403469765 ========> Accuracy: 85.40654077584196\n",
            "epochs: 24650 ========> Loss: 0.34307983080587684 ========> Accuracy: 85.41465840021918\n",
            "epochs: 24700 ========> Loss: 0.34305233312144096 ========> Accuracy: 85.41364369717202\n",
            "epochs: 24750 ========> Loss: 0.3430248961614221 ========> Accuracy: 85.41262899412487\n",
            "epochs: 24800 ========> Loss: 0.34299752556383506 ========> Accuracy: 85.41567310326633\n",
            "epochs: 24850 ========> Loss: 0.3429702270454319 ========> Accuracy: 85.41364369717202\n",
            "epochs: 24900 ========> Loss: 0.3429430060209563 ========> Accuracy: 85.41161429107773\n",
            "epochs: 24950 ========> Loss: 0.3429158672455645 ========> Accuracy: 85.41465840021918\n",
            "epochs: 25000 ========> Loss: 0.3428888145015945 ========> Accuracy: 85.41770250936064\n",
            "epochs: 25050 ========> Loss: 0.3428618503458427 ========> Accuracy: 85.41567310326633\n",
            "epochs: 25100 ========> Loss: 0.3428349759277576 ========> Accuracy: 85.41465840021918\n",
            "epochs: 25150 ========> Loss: 0.34280819088319053 ========> Accuracy: 85.41871721240778\n",
            "epochs: 25200 ========> Loss: 0.342781493303106 ========> Accuracy: 85.41465840021918\n",
            "epochs: 25250 ========> Loss: 0.34275487977225294 ========> Accuracy: 85.41465840021918\n",
            "epochs: 25300 ========> Loss: 0.3427283454694389 ========> Accuracy: 85.41770250936064\n",
            "epochs: 25350 ========> Loss: 0.3427018843187528 ========> Accuracy: 85.41668780631349\n",
            "epochs: 25400 ========> Loss: 0.34267548917981616 ========> Accuracy: 85.41871721240778\n",
            "epochs: 25450 ========> Loss: 0.3426491520647952 ========> Accuracy: 85.42176132154925\n",
            "epochs: 25500 ========> Loss: 0.3426228643702858 ========> Accuracy: 85.42886424287931\n",
            "epochs: 25550 ========> Loss: 0.3425966171131065 ========> Accuracy: 85.42987894592648\n",
            "epochs: 25600 ========> Loss: 0.34257040116029686 ========> Accuracy: 85.4207466185021\n",
            "epochs: 25650 ========> Loss: 0.342544207445036 ========> Accuracy: 85.4207466185021\n",
            "epochs: 25700 ========> Loss: 0.3425180271616587 ========> Accuracy: 85.42176132154925\n",
            "epochs: 25750 ========> Loss: 0.3424918519343797 ========> Accuracy: 85.42480543069071\n",
            "epochs: 25800 ========> Loss: 0.3424656739557257 ========> Accuracy: 85.42582013373786\n",
            "epochs: 25850 ========> Loss: 0.34243948609204344 ========> Accuracy: 85.43190835202078\n",
            "epochs: 25900 ========> Loss: 0.34241328195483883 ========> Accuracy: 85.43596716420939\n",
            "epochs: 25950 ========> Loss: 0.3423870559381232 ========> Accuracy: 85.4379965703037\n",
            "epochs: 26000 ========> Loss: 0.3423608032234123 ========> Accuracy: 85.44307008553946\n",
            "epochs: 26050 ========> Loss: 0.3423345197554748 ========> Accuracy: 85.44814360077523\n",
            "epochs: 26100 ========> Loss: 0.3423082021932943 ========> Accuracy: 85.45423181905815\n",
            "epochs: 26150 ========> Loss: 0.3422818478418741 ========> Accuracy: 85.45423181905815\n",
            "epochs: 26200 ========> Loss: 0.3422554545713534 ========> Accuracy: 85.46133474038822\n",
            "epochs: 26250 ========> Loss: 0.3422290207303275 ========> Accuracy: 85.47351117695406\n",
            "epochs: 26300 ========> Loss: 0.3422025450602093 ========> Accuracy: 85.47452588000122\n",
            "epochs: 26350 ========> Loss: 0.34217602661692087 ========> Accuracy: 85.46742295867115\n",
            "epochs: 26400 ========> Loss: 0.34214946470520075 ========> Accuracy: 85.46539355257684\n",
            "epochs: 26450 ========> Loss: 0.3421228588294536 ========> Accuracy: 85.46945236476546\n",
            "epochs: 26500 ========> Loss: 0.34209620866345297 ========> Accuracy: 85.4704670678126\n",
            "epochs: 26550 ========> Loss: 0.3420695140394819 ========> Accuracy: 85.4704670678126\n",
            "epochs: 26600 ========> Loss: 0.3420427749558069 ========> Accuracy: 85.4684376617183\n",
            "epochs: 26650 ========> Loss: 0.342015991599857 ========> Accuracy: 85.46539355257684\n",
            "epochs: 26700 ========> Loss: 0.3419891643832479 ========> Accuracy: 85.47452588000122\n",
            "epochs: 26750 ========> Loss: 0.34196229398394784 ========> Accuracy: 85.46945236476546\n",
            "epochs: 26800 ========> Loss: 0.34193538139046536 ========> Accuracy: 85.46437884952968\n",
            "epochs: 26850 ========> Loss: 0.3419084279429546 ========> Accuracy: 85.45930533429392\n",
            "epochs: 26900 ========> Loss: 0.3418814353665374 ========> Accuracy: 85.45930533429392\n",
            "epochs: 26950 ========> Loss: 0.341854405792837 ========> Accuracy: 85.46032003734108\n",
            "epochs: 27000 ========> Loss: 0.34182734176662366 ========> Accuracy: 85.46640825562399\n",
            "epochs: 27050 ========> Loss: 0.341800246235498 ========> Accuracy: 85.46945236476546\n",
            "epochs: 27100 ========> Loss: 0.34177312252161113 ========> Accuracy: 85.47351117695406\n",
            "epochs: 27150 ========> Loss: 0.3417459742755181 ========> Accuracy: 85.47452588000122\n",
            "epochs: 27200 ========> Loss: 0.3417188054133322 ========> Accuracy: 85.47655528609552\n",
            "epochs: 27250 ========> Loss: 0.34169162003936304 ========> Accuracy: 85.47858469218983\n",
            "epochs: 27300 ========> Loss: 0.34166442235731015 ========> Accuracy: 85.47351117695406\n",
            "epochs: 27350 ========> Loss: 0.34163721657376694 ========> Accuracy: 85.47452588000122\n",
            "epochs: 27400 ========> Loss: 0.34161000679818604 ========> Accuracy: 85.47452588000122\n",
            "epochs: 27450 ========> Loss: 0.3415827969435215 ========> Accuracy: 85.47351117695406\n",
            "epochs: 27500 ========> Loss: 0.3415555906315054 ========> Accuracy: 85.4704670678126\n",
            "epochs: 27550 ========> Loss: 0.3415283911060143 ========> Accuracy: 85.4684376617183\n",
            "epochs: 27600 ========> Loss: 0.34150120115737675 ========> Accuracy: 85.46945236476546\n",
            "epochs: 27650 ========> Loss: 0.34147402305988744 ========> Accuracy: 85.4704670678126\n",
            "epochs: 27700 ========> Loss: 0.34144685852437284 ========> Accuracy: 85.47249647390692\n",
            "epochs: 27750 ========> Loss: 0.3414197086674174 ========> Accuracy: 85.47655528609552\n",
            "epochs: 27800 ========> Loss: 0.34139257399881 ========> Accuracy: 85.46945236476546\n",
            "epochs: 27850 ========> Loss: 0.3413654544287929 ========> Accuracy: 85.46437884952968\n",
            "epochs: 27900 ========> Loss: 0.34133834929667645 ========> Accuracy: 85.46234944343539\n",
            "epochs: 27950 ========> Loss: 0.34131125742218693 ========> Accuracy: 85.46234944343539\n",
            "epochs: 28000 ========> Loss: 0.3412841771804473 ========> Accuracy: 85.46437884952968\n",
            "epochs: 28050 ========> Loss: 0.34125710660071834 ========> Accuracy: 85.45930533429392\n",
            "epochs: 28100 ========> Loss: 0.34123004348797725 ========> Accuracy: 85.45626122515246\n",
            "epochs: 28150 ========> Loss: 0.34120298556516404 ========> Accuracy: 85.45727592819962\n",
            "epochs: 28200 ========> Loss: 0.34117593063260215 ========> Accuracy: 85.45626122515246\n",
            "epochs: 28250 ========> Loss: 0.3411488767398239 ========> Accuracy: 85.453217116011\n",
            "epochs: 28300 ========> Loss: 0.34112182236389466 ========> Accuracy: 85.45930533429392\n",
            "epochs: 28350 ========> Loss: 0.3410947665873916 ========> Accuracy: 85.45829063124677\n",
            "epochs: 28400 ========> Loss: 0.3410677092684637 ========> Accuracy: 85.45524652210531\n",
            "epochs: 28450 ========> Loss: 0.34104065119486404 ========> Accuracy: 85.45423181905815\n",
            "epochs: 28500 ========> Loss: 0.3410135942135068 ========> Accuracy: 85.45829063124677\n",
            "epochs: 28550 ========> Loss: 0.3409865413270138 ========> Accuracy: 85.45829063124677\n",
            "epochs: 28600 ========> Loss: 0.34095949674903836 ========> Accuracy: 85.46336414648253\n",
            "epochs: 28650 ========> Loss: 0.3409324659110947 ========> Accuracy: 85.4704670678126\n",
            "epochs: 28700 ========> Loss: 0.340905455415461 ========> Accuracy: 85.46539355257684\n",
            "epochs: 28750 ========> Loss: 0.34087847293158974 ========> Accuracy: 85.46336414648253\n",
            "epochs: 28800 ========> Loss: 0.3408515270373253 ========> Accuracy: 85.46133474038822\n",
            "epochs: 28850 ========> Loss: 0.34082462701071425 ========> Accuracy: 85.46234944343539\n",
            "epochs: 28900 ========> Loss: 0.3407977825826654 ========> Accuracy: 85.46133474038822\n",
            "epochs: 28950 ========> Loss: 0.340771003664363 ========> Accuracy: 85.46437884952968\n",
            "epochs: 29000 ========> Loss: 0.34074430006543793 ========> Accuracy: 85.46742295867115\n",
            "epochs: 29050 ========> Loss: 0.3407176812190325 ========> Accuracy: 85.47655528609552\n",
            "epochs: 29100 ========> Loss: 0.34069115592810273 ========> Accuracy: 85.47959939523699\n",
            "epochs: 29150 ========> Loss: 0.3406647321440398 ========> Accuracy: 85.47959939523699\n",
            "epochs: 29200 ========> Loss: 0.3406384167847238 ========> Accuracy: 85.48365820742559\n",
            "epochs: 29250 ========> Loss: 0.3406122155952151 ========> Accuracy: 85.49380523789712\n",
            "epochs: 29300 ========> Loss: 0.3405861330510484 ========> Accuracy: 85.49380523789712\n",
            "epochs: 29350 ========> Loss: 0.34056017230183094 ========> Accuracy: 85.49684934703859\n",
            "epochs: 29400 ========> Loss: 0.34053433515156184 ========> Accuracy: 85.49989345618005\n",
            "epochs: 29450 ========> Loss: 0.34050862207158156 ========> Accuracy: 85.49076112875566\n",
            "epochs: 29500 ========> Loss: 0.3404830322420551 ========> Accuracy: 85.48974642570852\n",
            "epochs: 29550 ========> Loss: 0.34045756361811264 ========> Accuracy: 85.49076112875566\n",
            "epochs: 29600 ========> Loss: 0.3404322130170624 ========> Accuracy: 85.49786405008575\n",
            "epochs: 29650 ========> Loss: 0.3404069762233625 ========> Accuracy: 85.4988787531329\n",
            "epochs: 29700 ========> Loss: 0.3403818481083094 ========> Accuracy: 85.49583464399143\n",
            "epochs: 29750 ========> Loss: 0.34035682276167345 ========> Accuracy: 85.49583464399143\n",
            "epochs: 29800 ========> Loss: 0.3403318936328361 ========> Accuracy: 85.49279053484997\n",
            "epochs: 29850 ========> Loss: 0.340307053679305 ========> Accuracy: 85.49380523789712\n",
            "epochs: 29900 ========> Loss: 0.3402822955207811 ========> Accuracy: 85.50090815922721\n",
            "epochs: 29950 ========> Loss: 0.3402576115971201 ========> Accuracy: 85.50598167446297\n",
            "epochs: 30000 ========> Loss: 0.3402329943285105 ========> Accuracy: 85.50293756532152\n",
            "epochs: 30050 ========> Loss: 0.3402084362759162 ========> Accuracy: 85.50496697141583\n",
            "epochs: 30100 ========> Loss: 0.3401839302993082 ========> Accuracy: 85.50598167446297\n",
            "epochs: 30150 ========> Loss: 0.34015946971049776 ========> Accuracy: 85.51206989274588\n",
            "epochs: 30200 ========> Loss: 0.34013504841660347 ========> Accuracy: 85.5161287049345\n",
            "epochs: 30250 ========> Loss: 0.34011066104952636 ========> Accuracy: 85.52323162626458\n",
            "epochs: 30300 ========> Loss: 0.34008630307644133 ========> Accuracy: 85.52323162626458\n",
            "epochs: 30350 ========> Loss: 0.3400619708864373 ========> Accuracy: 85.52627573540603\n",
            "epochs: 30400 ========> Loss: 0.3400376618491385 ========> Accuracy: 85.5313492506418\n",
            "epochs: 30450 ========> Loss: 0.340013374342431 ========> Accuracy: 85.52830514150034\n",
            "epochs: 30500 ========> Loss: 0.33998910774820035 ========> Accuracy: 85.52221692321741\n",
            "epochs: 30550 ========> Loss: 0.339964862417047 ========> Accuracy: 85.52424632931172\n",
            "epochs: 30600 ========> Loss: 0.33994063960502063 ========> Accuracy: 85.52323162626458\n",
            "epochs: 30650 ========> Loss: 0.33991644138723515 ========> Accuracy: 85.52424632931172\n",
            "epochs: 30700 ========> Loss: 0.33989227055452503 ========> Accuracy: 85.53236395368896\n",
            "epochs: 30750 ========> Loss: 0.33986813049995457 ========> Accuracy: 85.53236395368896\n",
            "epochs: 30800 ========> Loss: 0.33984402510190614 ========> Accuracy: 85.53439335978325\n",
            "epochs: 30850 ========> Loss: 0.3398199586097055 ========> Accuracy: 85.53743746892472\n",
            "epochs: 30900 ========> Loss: 0.33979593553640797 ========> Accuracy: 85.53540806283041\n",
            "epochs: 30950 ========> Loss: 0.3397719605616369 ========> Accuracy: 85.54048157806618\n",
            "epochs: 31000 ========> Loss: 0.33974803844545615 ========> Accuracy: 85.5445403902548\n",
            "epochs: 31050 ========> Loss: 0.33972417395242044 ========> Accuracy: 85.54758449939625\n",
            "epochs: 31100 ========> Loss: 0.3397003717834223 ========> Accuracy: 85.55164331158487\n",
            "epochs: 31150 ========> Loss: 0.3396766365119771 ========> Accuracy: 85.55468742072632\n",
            "epochs: 31200 ========> Loss: 0.33965297252132687 ========> Accuracy: 85.55062860853772\n",
            "epochs: 31250 ========> Loss: 0.33962938393923275 ========> Accuracy: 85.55265801463202\n",
            "epochs: 31300 ========> Loss: 0.33960587456849733 ========> Accuracy: 85.55468742072632\n",
            "epochs: 31350 ========> Loss: 0.3395824478128658 ========> Accuracy: 85.55570212377349\n",
            "epochs: 31400 ========> Loss: 0.33955910659973065 ========> Accuracy: 85.55468742072632\n",
            "epochs: 31450 ========> Loss: 0.33953585330266456 ========> Accuracy: 85.55874623291494\n",
            "epochs: 31500 ========> Loss: 0.33951268966802006 ========> Accuracy: 85.55468742072632\n",
            "epochs: 31550 ========> Loss: 0.33948961675047895 ========> Accuracy: 85.5638197481507\n",
            "epochs: 31600 ========> Loss: 0.33946663486249506 ========> Accuracy: 85.55773152986778\n",
            "epochs: 31650 ========> Loss: 0.33944374354206863 ========> Accuracy: 85.55671682682063\n",
            "epochs: 31700 ========> Loss: 0.3394209415423119 ========> Accuracy: 85.56077563900925\n",
            "epochs: 31750 ========> Loss: 0.33939822684491844 ========> Accuracy: 85.55773152986778\n",
            "epochs: 31800 ========> Loss: 0.33937559669806805 ========> Accuracy: 85.55976093596209\n",
            "epochs: 31850 ========> Loss: 0.33935304767762237 ========> Accuracy: 85.55874623291494\n",
            "epochs: 31900 ========> Loss: 0.33933057576886 ========> Accuracy: 85.56077563900925\n",
            "epochs: 31950 ========> Loss: 0.33930817646462436 ========> Accuracy: 85.55976093596209\n",
            "epochs: 32000 ========> Loss: 0.339285844874749 ========> Accuracy: 85.56280504510354\n",
            "epochs: 32050 ========> Loss: 0.33926357584109385 ========> Accuracy: 85.55976093596209\n",
            "epochs: 32100 ========> Loss: 0.33924136405250105 ========> Accuracy: 85.55468742072632\n",
            "epochs: 32150 ========> Loss: 0.33921920415447526 ========> Accuracy: 85.55671682682063\n",
            "epochs: 32200 ========> Loss: 0.33919709084931887 ========> Accuracy: 85.55671682682063\n",
            "epochs: 32250 ========> Loss: 0.33917501898372343 ========> Accuracy: 85.55671682682063\n",
            "epochs: 32300 ========> Loss: 0.33915298362227053 ========> Accuracy: 85.55976093596209\n",
            "epochs: 32350 ========> Loss: 0.3391309801067696 ========> Accuracy: 85.55773152986778\n",
            "epochs: 32400 ========> Loss: 0.33910900410265404 ========> Accuracy: 85.55164331158487\n",
            "epochs: 32450 ========> Loss: 0.33908705163458697 ========> Accuracy: 85.55773152986778\n",
            "epochs: 32500 ========> Loss: 0.3390651191138266 ========> Accuracy: 85.56077563900925\n",
            "epochs: 32550 ========> Loss: 0.33904320335966465 ========> Accuracy: 85.56077563900925\n",
            "epochs: 32600 ========> Loss: 0.339021301616365 ========> Accuracy: 85.56280504510354\n",
            "epochs: 32650 ========> Loss: 0.3389994115655927 ========> Accuracy: 85.56280504510354\n",
            "epochs: 32700 ========> Loss: 0.3389775313325476 ========> Accuracy: 85.5638197481507\n",
            "epochs: 32750 ========> Loss: 0.3389556594822219 ========> Accuracy: 85.56584915424502\n",
            "epochs: 32800 ========> Loss: 0.33893379500075443 ========> Accuracy: 85.5638197481507\n",
            "epochs: 32850 ========> Loss: 0.3389119372561339 ========> Accuracy: 85.5638197481507\n",
            "epochs: 32900 ========> Loss: 0.3388900859328081 ========> Accuracy: 85.56280504510354\n",
            "epochs: 32950 ========> Loss: 0.338868240936252 ========> Accuracy: 85.5617903420564\n",
            "epochs: 33000 ========> Loss: 0.33884640226619545 ========> Accuracy: 85.56686385729216\n",
            "epochs: 33050 ========> Loss: 0.3388245698607881 ========> Accuracy: 85.5638197481507\n",
            "epochs: 33100 ========> Loss: 0.33880274341804334 ========> Accuracy: 85.55874623291494\n",
            "epochs: 33150 ========> Loss: 0.3387809222049629 ========> Accuracy: 85.55671682682063\n",
            "epochs: 33200 ========> Loss: 0.3387591048682745 ========> Accuracy: 85.56889326338647\n",
            "epochs: 33250 ========> Loss: 0.3387372892633668 ========> Accuracy: 85.56787856033932\n",
            "epochs: 33300 ========> Loss: 0.3387154723196635 ========> Accuracy: 85.57295207557509\n",
            "epochs: 33350 ========> Loss: 0.338693649961413 ========> Accuracy: 85.57092266948078\n",
            "epochs: 33400 ========> Loss: 0.3386718171030371 ========> Accuracy: 85.57295207557509\n",
            "epochs: 33450 ========> Loss: 0.3386499677380463 ========> Accuracy: 85.57396677862224\n",
            "epochs: 33500 ========> Loss: 0.3386280951401991 ========> Accuracy: 85.57295207557509\n",
            "epochs: 33550 ========> Loss: 0.3386061921946709 ========> Accuracy: 85.57396677862224\n",
            "epochs: 33600 ========> Loss: 0.3385842518743511 ========> Accuracy: 85.57295207557509\n",
            "epochs: 33650 ========> Loss: 0.3385622678700379 ========> Accuracy: 85.57498148166938\n",
            "epochs: 33700 ========> Loss: 0.3385402353705834 ========> Accuracy: 85.57701088776369\n",
            "epochs: 33750 ========> Loss: 0.3385181519673105 ========> Accuracy: 85.57193737252793\n",
            "epochs: 33800 ========> Loss: 0.3384960186251213 ========> Accuracy: 85.57599618471654\n",
            "epochs: 33850 ========> Loss: 0.33847384062310526 ========> Accuracy: 85.58005499690515\n",
            "epochs: 33900 ========> Loss: 0.3384516283283894 ========> Accuracy: 85.58411380909376\n",
            "epochs: 33950 ========> Loss: 0.3384293976431318 ========> Accuracy: 85.58512851214091\n",
            "epochs: 34000 ========> Loss: 0.33840716997377035 ========> Accuracy: 85.58512851214091\n",
            "epochs: 34050 ========> Loss: 0.3383849716268936 ========> Accuracy: 85.58512851214091\n",
            "epochs: 34100 ========> Loss: 0.3383628326351937 ========> Accuracy: 85.58208440299946\n",
            "epochs: 34150 ========> Loss: 0.3383407851364656 ========> Accuracy: 85.58715791823522\n",
            "epochs: 34200 ========> Loss: 0.3383188615295413 ========> Accuracy: 85.58614321518807\n",
            "epochs: 34250 ========> Loss: 0.33829709267634245 ========> Accuracy: 85.58005499690515\n",
            "epochs: 34300 ========> Loss: 0.3382755063933353 ========> Accuracy: 85.58208440299946\n",
            "epochs: 34350 ========> Loss: 0.338254126392412 ========> Accuracy: 85.57701088776369\n",
            "epochs: 34400 ========> Loss: 0.3382329717245959 ========> Accuracy: 85.57295207557509\n",
            "epochs: 34450 ========> Loss: 0.3382120566854532 ========> Accuracy: 85.56787856033932\n",
            "epochs: 34500 ========> Loss: 0.3381913910805244 ========> Accuracy: 85.56280504510354\n",
            "epochs: 34550 ========> Loss: 0.33817098072669005 ========> Accuracy: 85.55976093596209\n",
            "epochs: 34600 ========> Loss: 0.3381508280727394 ========> Accuracy: 85.56077563900925\n",
            "epochs: 34650 ========> Loss: 0.3381309328465906 ========> Accuracy: 85.56787856033932\n",
            "epochs: 34700 ========> Loss: 0.33811129266604756 ========> Accuracy: 85.56787856033932\n",
            "epochs: 34750 ========> Loss: 0.3380919035770758 ========> Accuracy: 85.57092266948078\n",
            "epochs: 34800 ========> Loss: 0.3380727605047088 ========> Accuracy: 85.56787856033932\n",
            "epochs: 34850 ========> Loss: 0.33805385761607337 ========> Accuracy: 85.57092266948078\n",
            "epochs: 34900 ========> Loss: 0.33803518860345516 ========> Accuracy: 85.56889326338647\n",
            "epochs: 34950 ========> Loss: 0.3380167468992452 ========> Accuracy: 85.57396677862224\n",
            "epochs: 35000 ========> Loss: 0.3379985258355029 ========> Accuracy: 85.58005499690515\n",
            "epochs: 35050 ========> Loss: 0.3379805187599634 ========> Accuracy: 85.58208440299946\n",
            "epochs: 35100 ========> Loss: 0.3379627191185557 ========> Accuracy: 85.58309910604662\n",
            "epochs: 35150 ========> Loss: 0.337945120512472 ========> Accuracy: 85.58106969995231\n",
            "epochs: 35200 ========> Loss: 0.33792771673593053 ========> Accuracy: 85.58005499690515\n",
            "epochs: 35250 ========> Loss: 0.3379105017991757 ========> Accuracy: 85.58005499690515\n",
            "epochs: 35300 ========> Loss: 0.33789346994004277 ========> Accuracy: 85.579040293858\n",
            "epochs: 35350 ========> Loss: 0.3378766156265602 ========> Accuracy: 85.57092266948078\n",
            "epochs: 35400 ========> Loss: 0.33785993355252647 ========> Accuracy: 85.56990796643362\n",
            "epochs: 35450 ========> Loss: 0.33784341862770656 ========> Accuracy: 85.56787856033932\n",
            "epochs: 35500 ========> Loss: 0.3378270659641751 ========> Accuracy: 85.56686385729216\n",
            "epochs: 35550 ========> Loss: 0.3378108708603143 ========> Accuracy: 85.56990796643362\n",
            "epochs: 35600 ========> Loss: 0.3377948287839968 ========> Accuracy: 85.56990796643362\n",
            "epochs: 35650 ========> Loss: 0.3377789353565076 ========> Accuracy: 85.57396677862224\n",
            "epochs: 35700 ========> Loss: 0.3377631863387326 ========> Accuracy: 85.57802559081085\n",
            "epochs: 35750 ========> Loss: 0.33774757762105984 ========> Accuracy: 85.59020202737668\n",
            "epochs: 35800 ========> Loss: 0.3377321052182768 ========> Accuracy: 85.59324613651815\n",
            "epochs: 35850 ========> Loss: 0.3377167652704927 ========> Accuracy: 85.59020202737668\n",
            "epochs: 35900 ========> Loss: 0.337701554050777 ========> Accuracy: 85.59121673042385\n",
            "epochs: 35950 ========> Loss: 0.3376864679797707 ========> Accuracy: 85.592231433471\n",
            "epochs: 36000 ========> Loss: 0.33767150364702014 ========> Accuracy: 85.58715791823522\n",
            "epochs: 36050 ========> Loss: 0.33765665783819226 ========> Accuracy: 85.58411380909376\n",
            "epochs: 36100 ========> Loss: 0.33764192756670974 ========> Accuracy: 85.58715791823522\n",
            "epochs: 36150 ========> Loss: 0.3376273101076971 ========> Accuracy: 85.592231433471\n",
            "epochs: 36200 ========> Loss: 0.3376128030315405 ========> Accuracy: 85.59730494870676\n",
            "epochs: 36250 ========> Loss: 0.3375984042338744 ========> Accuracy: 85.60237846394253\n",
            "epochs: 36300 ========> Loss: 0.33758411195852706 ========> Accuracy: 85.60440787003684\n",
            "epochs: 36350 ========> Loss: 0.33756992480994985 ========> Accuracy: 85.60440787003684\n",
            "epochs: 36400 ========> Loss: 0.33755584175200853 ========> Accuracy: 85.60237846394253\n",
            "epochs: 36450 ========> Loss: 0.3375418620907604 ========> Accuracy: 85.60034905784822\n",
            "epochs: 36500 ========> Loss: 0.3375279854399653 ========> Accuracy: 85.60339316698969\n",
            "epochs: 36550 ========> Loss: 0.33751421166949497 ========> Accuracy: 85.60339316698969\n",
            "epochs: 36600 ========> Loss: 0.33750054083836384 ========> Accuracy: 85.60237846394253\n",
            "epochs: 36650 ========> Loss: 0.33748697311558773 ========> Accuracy: 85.60136376089538\n",
            "epochs: 36700 ========> Loss: 0.3374735086932859 ========> Accuracy: 85.60643727613115\n",
            "epochs: 36750 ========> Loss: 0.3374601476971686 ========> Accuracy: 85.60542257308398\n",
            "epochs: 36800 ========> Loss: 0.33744689009970186 ========> Accuracy: 85.60643727613115\n",
            "epochs: 36850 ========> Loss: 0.3374337356407916 ========> Accuracy: 85.6094813852726\n",
            "epochs: 36900 ========> Loss: 0.337420683759882 ========> Accuracy: 85.61049608831975\n",
            "epochs: 36950 ========> Loss: 0.33740773354208536 ========> Accuracy: 85.6094813852726\n",
            "epochs: 37000 ========> Loss: 0.3373948836795654 ========> Accuracy: 85.60542257308398\n",
            "epochs: 37050 ========> Loss: 0.3373821324480728 ========> Accuracy: 85.61049608831975\n",
            "epochs: 37100 ========> Loss: 0.3373694776974493 ========> Accuracy: 85.60440787003684\n",
            "epochs: 37150 ========> Loss: 0.3373569168541411 ========> Accuracy: 85.60643727613115\n",
            "epochs: 37200 ========> Loss: 0.33734444693333776 ========> Accuracy: 85.6094813852726\n",
            "epochs: 37250 ========> Loss: 0.33733206455821985 ========> Accuracy: 85.61252549441406\n",
            "epochs: 37300 ========> Loss: 0.3373197659839152 ========> Accuracy: 85.61049608831975\n",
            "epochs: 37350 ========> Loss: 0.33730754712403876 ========> Accuracy: 85.61151079136691\n",
            "epochs: 37400 ========> Loss: 0.3372954035780478 ========> Accuracy: 85.60846668222545\n",
            "epochs: 37450 ========> Loss: 0.33728333065803423 ========> Accuracy: 85.60846668222545\n",
            "epochs: 37500 ========> Loss: 0.3372713234139416 ========> Accuracy: 85.6074519791783\n",
            "epochs: 37550 ========> Loss: 0.3372593766565203 ========> Accuracy: 85.61556960355551\n",
            "epochs: 37600 ========> Loss: 0.3372474849775971 ========> Accuracy: 85.61049608831975\n",
            "epochs: 37650 ========> Loss: 0.33723564276744156 ========> Accuracy: 85.60846668222545\n",
            "epochs: 37700 ========> Loss: 0.33722384422914703 ========> Accuracy: 85.6074519791783\n",
            "epochs: 37750 ========> Loss: 0.33721208339003556 ========> Accuracy: 85.60846668222545\n",
            "epochs: 37800 ========> Loss: 0.33720035411012206 ========> Accuracy: 85.60643727613115\n",
            "epochs: 37850 ========> Loss: 0.337188650087667 ========> Accuracy: 85.60440787003684\n",
            "epochs: 37900 ========> Loss: 0.33717696486179033 ========> Accuracy: 85.60643727613115\n",
            "epochs: 37950 ========> Loss: 0.33716529181204086 ========> Accuracy: 85.6074519791783\n",
            "epochs: 38000 ========> Loss: 0.33715362415470146 ========> Accuracy: 85.60339316698969\n",
            "epochs: 38050 ========> Loss: 0.33714195493549526 ========> Accuracy: 85.60136376089538\n",
            "epochs: 38100 ========> Loss: 0.33713027701823456 ========> Accuracy: 85.59933435480107\n",
            "epochs: 38150 ========> Loss: 0.33711858306885606 ========> Accuracy: 85.59527554261246\n",
            "epochs: 38200 ========> Loss: 0.33710686553424585 ========> Accuracy: 85.592231433471\n",
            "epochs: 38250 ========> Loss: 0.33709511661529323 ========> Accuracy: 85.59020202737668\n",
            "epochs: 38300 ========> Loss: 0.3370833282337946 ========> Accuracy: 85.59121673042385\n",
            "epochs: 38350 ========> Loss: 0.3370714919932021 ========> Accuracy: 85.58715791823522\n",
            "epochs: 38400 ========> Loss: 0.3370595991338374 ========> Accuracy: 85.58817262128238\n",
            "epochs: 38450 ========> Loss: 0.33704764048414937 ========> Accuracy: 85.59121673042385\n",
            "epochs: 38500 ========> Loss: 0.3370356064109211 ========> Accuracy: 85.58715791823522\n",
            "epochs: 38550 ========> Loss: 0.33702348677307503 ========> Accuracy: 85.58614321518807\n",
            "epochs: 38600 ========> Loss: 0.33701127088584615 ========> Accuracy: 85.59020202737668\n",
            "epochs: 38650 ========> Loss: 0.33699894750448894 ========> Accuracy: 85.59629024565962\n",
            "epochs: 38700 ========> Loss: 0.3369865048391169 ========> Accuracy: 85.59426083956531\n",
            "epochs: 38750 ========> Loss: 0.3369739306143447 ========> Accuracy: 85.58918732432953\n",
            "epochs: 38800 ========> Loss: 0.3369612121885275 ========> Accuracy: 85.59426083956531\n",
            "epochs: 38850 ========> Loss: 0.33694833674679775 ========> Accuracy: 85.59324613651815\n",
            "epochs: 38900 ========> Loss: 0.3369352915789288 ========> Accuracy: 85.59527554261246\n",
            "epochs: 38950 ========> Loss: 0.33692206444649664 ========> Accuracy: 85.59831965175391\n",
            "epochs: 39000 ========> Loss: 0.33690864403341003 ========> Accuracy: 85.60339316698969\n",
            "epochs: 39050 ========> Loss: 0.3368950204598393 ========> Accuracy: 85.60440787003684\n",
            "epochs: 39100 ========> Loss: 0.3368811858231519 ========> Accuracy: 85.60643727613115\n",
            "epochs: 39150 ========> Loss: 0.33686713471312046 ========> Accuracy: 85.61049608831975\n",
            "epochs: 39200 ========> Loss: 0.3368528646360006 ========> Accuracy: 85.61252549441406\n",
            "epochs: 39250 ========> Loss: 0.3368383762772506 ========> Accuracy: 85.61151079136691\n",
            "epochs: 39300 ========> Loss: 0.33682367353947523 ========> Accuracy: 85.60846668222545\n",
            "epochs: 39350 ========> Loss: 0.33680876331277404 ========> Accuracy: 85.60846668222545\n",
            "epochs: 39400 ========> Loss: 0.3367936549682773 ========> Accuracy: 85.60846668222545\n",
            "epochs: 39450 ========> Loss: 0.3367783596077856 ========> Accuracy: 85.60643727613115\n",
            "epochs: 39500 ========> Loss: 0.33676288914501257 ========> Accuracy: 85.60542257308398\n",
            "epochs: 39550 ========> Loss: 0.33674725532690736 ========> Accuracy: 85.61049608831975\n",
            "epochs: 39600 ========> Loss: 0.3367314688176713 ========> Accuracy: 85.61151079136691\n",
            "epochs: 39650 ========> Loss: 0.3367155384581783 ========> Accuracy: 85.61252549441406\n",
            "epochs: 39700 ========> Loss: 0.33669947078045165 ========> Accuracy: 85.61354019746122\n",
            "epochs: 39750 ========> Loss: 0.33668326980757574 ========> Accuracy: 85.61759900964982\n",
            "epochs: 39800 ========> Loss: 0.33666693711477863 ========> Accuracy: 85.61556960355551\n",
            "epochs: 39850 ========> Loss: 0.33665047207932747 ========> Accuracy: 85.62064311879128\n",
            "epochs: 39900 ========> Loss: 0.33663387221595864 ========> Accuracy: 85.61861371269698\n",
            "epochs: 39950 ========> Loss: 0.33661713348874556 ========> Accuracy: 85.62977544621566\n",
            "epochs: 40000 ========> Loss: 0.3366002505138934 ========> Accuracy: 85.62977544621566\n",
            "epochs: 40050 ========> Loss: 0.3365832166201872 ========> Accuracy: 85.62876074316851\n",
            "epochs: 40100 ========> Loss: 0.33656602380732353 ========> Accuracy: 85.62977544621566\n",
            "epochs: 40150 ========> Loss: 0.3365486627223607 ========> Accuracy: 85.62571663402704\n",
            "epochs: 40200 ========> Loss: 0.3365311228406416 ========> Accuracy: 85.61658430660268\n",
            "epochs: 40250 ========> Loss: 0.33651339306838385 ========> Accuracy: 85.61962841574413\n",
            "epochs: 40300 ========> Loss: 0.33649546296354726 ========> Accuracy: 85.61759900964982\n",
            "epochs: 40350 ========> Loss: 0.3364773246942039 ========> Accuracy: 85.61962841574413\n",
            "epochs: 40400 ========> Loss: 0.3364589757268063 ========> Accuracy: 85.61556960355551\n",
            "epochs: 40450 ========> Loss: 0.3364404220764457 ========> Accuracy: 85.61252549441406\n",
            "epochs: 40500 ========> Loss: 0.336421681771818 ========> Accuracy: 85.61658430660268\n",
            "epochs: 40550 ========> Loss: 0.33640278799383283 ========> Accuracy: 85.61354019746122\n",
            "epochs: 40600 ========> Loss: 0.33638379114753547 ========> Accuracy: 85.61252549441406\n",
            "epochs: 40650 ========> Loss: 0.3363647589846375 ========> Accuracy: 85.61556960355551\n",
            "epochs: 40700 ========> Loss: 0.33634577396938936 ========> Accuracy: 85.61556960355551\n",
            "epochs: 40750 ========> Loss: 0.33632692757161103 ========> Accuracy: 85.61556960355551\n",
            "epochs: 40800 ========> Loss: 0.3363083120949238 ========> Accuracy: 85.61252549441406\n",
            "epochs: 40850 ========> Loss: 0.3362900116492257 ========> Accuracy: 85.61455490050837\n",
            "epochs: 40900 ========> Loss: 0.3362720943631747 ========> Accuracy: 85.61455490050837\n",
            "epochs: 40950 ========> Loss: 0.33625460754855035 ========> Accuracy: 85.61556960355551\n",
            "epochs: 41000 ========> Loss: 0.3362375764819055 ========> Accuracy: 85.61151079136691\n",
            "epochs: 41050 ========> Loss: 0.33622100634954927 ========> Accuracy: 85.60643727613115\n",
            "epochs: 41100 ========> Loss: 0.3362048862117382 ========> Accuracy: 85.6074519791783\n",
            "epochs: 41150 ========> Loss: 0.33618919371829853 ========> Accuracy: 85.60643727613115\n",
            "epochs: 41200 ========> Loss: 0.33617389958638955 ========> Accuracy: 85.60542257308398\n",
            "epochs: 41250 ========> Loss: 0.33615897127571637 ========> Accuracy: 85.6074519791783\n",
            "epochs: 41300 ========> Loss: 0.3361443756747742 ========> Accuracy: 85.61151079136691\n",
            "epochs: 41350 ========> Loss: 0.3361300808601212 ========> Accuracy: 85.61049608831975\n",
            "epochs: 41400 ========> Loss: 0.33611605710892956 ========> Accuracy: 85.61759900964982\n",
            "epochs: 41450 ========> Loss: 0.33610227737129167 ========> Accuracy: 85.61962841574413\n",
            "epochs: 41500 ========> Loss: 0.33608871738556423 ========> Accuracy: 85.61354019746122\n",
            "epochs: 41550 ========> Loss: 0.33607535557881785 ========> Accuracy: 85.61759900964982\n",
            "epochs: 41600 ========> Loss: 0.33606217285300893 ========> Accuracy: 85.61556960355551\n",
            "epochs: 41650 ========> Loss: 0.336049152323247 ========> Accuracy: 85.61354019746122\n",
            "epochs: 41700 ========> Loss: 0.3360362790491134 ========> Accuracy: 85.61455490050837\n",
            "epochs: 41750 ========> Loss: 0.33602353978230665 ========> Accuracy: 85.61658430660268\n",
            "epochs: 41800 ========> Loss: 0.3360109227421543 ========> Accuracy: 85.61252549441406\n",
            "epochs: 41850 ========> Loss: 0.33599841742301084 ========> Accuracy: 85.61354019746122\n",
            "epochs: 41900 ========> Loss: 0.3359860144329716 ========> Accuracy: 85.61354019746122\n",
            "epochs: 41950 ========> Loss: 0.3359737053607223 ========> Accuracy: 85.61658430660268\n",
            "epochs: 42000 ========> Loss: 0.3359614826661061 ========> Accuracy: 85.61962841574413\n",
            "epochs: 42050 ========> Loss: 0.3359493395896888 ========> Accuracy: 85.6247019309799\n",
            "epochs: 42100 ========> Loss: 0.33593727007693197 ========> Accuracy: 85.62774604012135\n",
            "epochs: 42150 ========> Loss: 0.3359252687132962 ========> Accuracy: 85.62267252488559\n",
            "epochs: 42200 ========> Loss: 0.3359133306675015 ========> Accuracy: 85.62064311879128\n",
            "epochs: 42250 ========> Loss: 0.33590145164109364 ========> Accuracy: 85.6247019309799\n",
            "epochs: 42300 ========> Loss: 0.33588962782329274 ========> Accuracy: 85.62876074316851\n",
            "epochs: 42350 ========> Loss: 0.3358778558507254 ========> Accuracy: 85.63079014926281\n",
            "epochs: 42400 ========> Loss: 0.3358661327720444 ========> Accuracy: 85.62774604012135\n",
            "epochs: 42450 ========> Loss: 0.33585445601760033 ========> Accuracy: 85.6267313370742\n",
            "epochs: 42500 ========> Loss: 0.3358428233742898 ========> Accuracy: 85.62267252488559\n",
            "epochs: 42550 ========> Loss: 0.3358312329655252 ========> Accuracy: 85.62064311879128\n",
            "epochs: 42600 ========> Loss: 0.335819683236013 ========> Accuracy: 85.62571663402704\n",
            "epochs: 42650 ========> Loss: 0.33580817294075016 ========> Accuracy: 85.62267252488559\n",
            "epochs: 42700 ========> Loss: 0.33579670113741655 ========> Accuracy: 85.62165782183844\n",
            "epochs: 42750 ========> Loss: 0.33578526718116747 ========> Accuracy: 85.62165782183844\n",
            "epochs: 42800 ========> Loss: 0.335773870720738 ========> Accuracy: 85.61962841574413\n",
            "epochs: 42850 ========> Loss: 0.3357625116947602 ========> Accuracy: 85.62064311879128\n",
            "epochs: 42900 ========> Loss: 0.3357511903272449 ========> Accuracy: 85.62368722793275\n",
            "epochs: 42950 ========> Loss: 0.3357399071212803 ========> Accuracy: 85.61962841574413\n",
            "epochs: 43000 ========> Loss: 0.3357286628501427 ========> Accuracy: 85.62064311879128\n",
            "epochs: 43050 ========> Loss: 0.3357174585451619 ========> Accuracy: 85.62064311879128\n",
            "epochs: 43100 ========> Loss: 0.3357062954798614 ========> Accuracy: 85.62368722793275\n",
            "epochs: 43150 ========> Loss: 0.33569517515005903 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43200 ========> Loss: 0.3356840992497878 ========> Accuracy: 85.62571663402704\n",
            "epochs: 43250 ========> Loss: 0.33567306964307136 ========> Accuracy: 85.62876074316851\n",
            "epochs: 43300 ========> Loss: 0.33566208833175876 ========> Accuracy: 85.62571663402704\n",
            "epochs: 43350 ========> Loss: 0.3356511574197909 ========> Accuracy: 85.62876074316851\n",
            "epochs: 43400 ========> Loss: 0.335640279074433 ========> Accuracy: 85.6267313370742\n",
            "epochs: 43450 ========> Loss: 0.3356294554851625 ========> Accuracy: 85.63281955535712\n",
            "epochs: 43500 ========> Loss: 0.33561868882103135 ========> Accuracy: 85.63484896145142\n",
            "epochs: 43550 ========> Loss: 0.3356079811874364 ========> Accuracy: 85.63484896145142\n",
            "epochs: 43600 ========> Loss: 0.3355973345833061 ========> Accuracy: 85.63180485230997\n",
            "epochs: 43650 ========> Loss: 0.33558675085975326 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43700 ========> Loss: 0.3355762316812331 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43750 ========> Loss: 0.33556577849019376 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43800 ========> Loss: 0.3355553924761036 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43850 ========> Loss: 0.33554507454959454 ========> Accuracy: 85.6247019309799\n",
            "epochs: 43900 ========> Loss: 0.335534825322282 ========> Accuracy: 85.62774604012135\n",
            "epochs: 43950 ========> Loss: 0.3355246450926163 ========> Accuracy: 85.62571663402704\n",
            "epochs: 44000 ========> Loss: 0.3355145338379052 ========> Accuracy: 85.62267252488559\n",
            "epochs: 44050 ========> Loss: 0.3355044912124298 ========> Accuracy: 85.62368722793275\n",
            "epochs: 44100 ========> Loss: 0.33549451655137397 ========> Accuracy: 85.62774604012135\n",
            "epochs: 44150 ========> Loss: 0.3354846088801051 ========> Accuracy: 85.6247019309799\n",
            "epochs: 44200 ========> Loss: 0.33547476692819894 ========> Accuracy: 85.6247019309799\n",
            "epochs: 44250 ========> Loss: 0.3354649891474883 ========> Accuracy: 85.6247019309799\n",
            "epochs: 44300 ========> Loss: 0.3354552737333469 ========> Accuracy: 85.62571663402704\n",
            "epochs: 44350 ========> Loss: 0.3354456186483911 ========> Accuracy: 85.62774604012135\n",
            "epochs: 44400 ========> Loss: 0.3354360216477937 ========> Accuracy: 85.6378930705929\n",
            "epochs: 44450 ========> Loss: 0.335426480305449 ========> Accuracy: 85.63890777364006\n",
            "epochs: 44500 ========> Loss: 0.33541699204030806 ========> Accuracy: 85.6378930705929\n",
            "epochs: 44550 ========> Loss: 0.3354075541422997 ========> Accuracy: 85.63890777364006\n",
            "epochs: 44600 ========> Loss: 0.3353981637973782 ========> Accuracy: 85.64093717973435\n",
            "epochs: 44650 ========> Loss: 0.3353888181113644 ========> Accuracy: 85.64093717973435\n",
            "epochs: 44700 ========> Loss: 0.3353795141323889 ========> Accuracy: 85.64093717973435\n",
            "epochs: 44750 ========> Loss: 0.3353702488718808 ========> Accuracy: 85.63484896145142\n",
            "epochs: 44800 ========> Loss: 0.33536101932418333 ========> Accuracy: 85.63079014926281\n",
            "epochs: 44850 ========> Loss: 0.33535182248500334 ========> Accuracy: 85.63180485230997\n",
            "epochs: 44900 ========> Loss: 0.3353426553690196 ========> Accuracy: 85.63281955535712\n",
            "epochs: 44950 ========> Loss: 0.3353335150270763 ========> Accuracy: 85.63383425840428\n",
            "epochs: 45000 ========> Loss: 0.3353243985634755 ========> Accuracy: 85.63484896145142\n",
            "epochs: 45050 ========> Loss: 0.33531530315394353 ========> Accuracy: 85.6378930705929\n",
            "epochs: 45100 ========> Loss: 0.3353062260648856 ========> Accuracy: 85.63383425840428\n",
            "epochs: 45150 ========> Loss: 0.3352971646745433 ========> Accuracy: 85.6378930705929\n",
            "epochs: 45200 ========> Loss: 0.3352881164966335 ========> Accuracy: 85.63586366449857\n",
            "epochs: 45250 ========> Loss: 0.3352790792069557 ========> Accuracy: 85.63281955535712\n",
            "epochs: 45300 ========> Loss: 0.3352700506733032 ========> Accuracy: 85.63180485230997\n",
            "epochs: 45350 ========> Loss: 0.3352610289887925 ========> Accuracy: 85.63890777364006\n",
            "epochs: 45400 ========> Loss: 0.3352520125084251 ========> Accuracy: 85.6378930705929\n",
            "epochs: 45450 ========> Loss: 0.3352429998883167 ========> Accuracy: 85.64398128887582\n",
            "epochs: 45500 ========> Loss: 0.33523399012658356 ========> Accuracy: 85.64398128887582\n",
            "epochs: 45550 ========> Loss: 0.3352249826043768 ========> Accuracy: 85.64195188278151\n",
            "epochs: 45600 ========> Loss: 0.3352159771250479 ========> Accuracy: 85.64195188278151\n",
            "epochs: 45650 ========> Loss: 0.3352069739489629 ========> Accuracy: 85.64601069497012\n",
            "epochs: 45700 ========> Loss: 0.33519797382112426 ========> Accuracy: 85.64804010106442\n",
            "epochs: 45750 ========> Loss: 0.3351889779885907 ========> Accuracy: 85.64804010106442\n",
            "epochs: 45800 ========> Loss: 0.3351799882047803 ========> Accuracy: 85.65108421020588\n",
            "epochs: 45850 ========> Loss: 0.335171006718147 ========> Accuracy: 85.65108421020588\n",
            "epochs: 45900 ========> Loss: 0.3351620362434715 ========> Accuracy: 85.65615772544164\n",
            "epochs: 45950 ========> Loss: 0.3351530799150696 ========> Accuracy: 85.6551430223945\n",
            "epochs: 46000 ========> Loss: 0.3351441412225063 ========> Accuracy: 85.65615772544164\n",
            "epochs: 46050 ========> Loss: 0.33513522393079476 ========> Accuracy: 85.6551430223945\n",
            "epochs: 46100 ========> Loss: 0.33512633198836966 ========> Accuracy: 85.66123124067741\n",
            "epochs: 46150 ========> Loss: 0.335117469427205 ========> Accuracy: 85.65920183458311\n",
            "epochs: 46200 ========> Loss: 0.3351086402601308 ========> Accuracy: 85.66224594372457\n",
            "epochs: 46250 ========> Loss: 0.3350998483806214 ========> Accuracy: 85.65311361630019\n",
            "epochs: 46300 ========> Loss: 0.3350910974700453 ========> Accuracy: 85.65412831934735\n",
            "epochs: 46350 ========> Loss: 0.33508239091665193 ========> Accuracy: 85.6551430223945\n",
            "epochs: 46400 ========> Loss: 0.3350737317495186 ========> Accuracy: 85.65006950715873\n",
            "epochs: 46450 ========> Loss: 0.33506512258944854 ========> Accuracy: 85.65108421020588\n",
            "epochs: 46500 ========> Loss: 0.3350565656175411 ========> Accuracy: 85.65108421020588\n",
            "epochs: 46550 ========> Loss: 0.33504806256098096 ========> Accuracy: 85.65108421020588\n",
            "epochs: 46600 ========> Loss: 0.33503961469460936 ========> Accuracy: 85.65209891325304\n",
            "epochs: 46650 ========> Loss: 0.3350312228561138 ========> Accuracy: 85.65006950715873\n",
            "epochs: 46700 ========> Loss: 0.33502288747221165 ========> Accuracy: 85.65311361630019\n",
            "epochs: 46750 ========> Loss: 0.3350146085930017 ========> Accuracy: 85.65006950715873\n",
            "epochs: 46800 ========> Loss: 0.33500638593168786 ========> Accuracy: 85.65006950715873\n",
            "epochs: 46850 ========> Loss: 0.3349982189070749 ========> Accuracy: 85.65006950715873\n",
            "epochs: 46900 ========> Loss: 0.33499010668657 ========> Accuracy: 85.64804010106442\n",
            "epochs: 46950 ========> Loss: 0.3349820482278226 ========> Accuracy: 85.65108421020588\n",
            "epochs: 47000 ========> Loss: 0.33497404231757255 ========> Accuracy: 85.65006950715873\n",
            "epochs: 47050 ========> Loss: 0.334966087606699 ========> Accuracy: 85.65615772544164\n",
            "epochs: 47100 ========> Loss: 0.334958182640865 ========> Accuracy: 85.6571724284888\n",
            "epochs: 47150 ========> Loss: 0.3349503258864881 ========> Accuracy: 85.66021653763026\n",
            "epochs: 47200 ========> Loss: 0.3349425157520575 ========> Accuracy: 85.66529005286603\n",
            "epochs: 47250 ========> Loss: 0.33493475060503447 ========> Accuracy: 85.66833416200748\n",
            "epochs: 47300 ========> Loss: 0.3349270287847318 ========> Accuracy: 85.66529005286603\n",
            "epochs: 47350 ========> Loss: 0.33491934861166983 ========> Accuracy: 85.66224594372457\n",
            "epochs: 47400 ========> Loss: 0.3349117083939622 ========> Accuracy: 85.66021653763026\n",
            "epochs: 47450 ========> Loss: 0.33490410643129875 ========> Accuracy: 85.65615772544164\n",
            "epochs: 47500 ========> Loss: 0.3348965410170818 ========> Accuracy: 85.65920183458311\n",
            "epochs: 47550 ========> Loss: 0.3348890104392339 ========> Accuracy: 85.6571724284888\n",
            "epochs: 47600 ========> Loss: 0.33488151298015306 ========> Accuracy: 85.6571724284888\n",
            "epochs: 47650 ========> Loss: 0.3348740469162336 ========> Accuracy: 85.6551430223945\n",
            "epochs: 47700 ========> Loss: 0.3348666105173184 ========> Accuracy: 85.65920183458311\n",
            "epochs: 47750 ========> Loss: 0.33485920204639424 ========> Accuracy: 85.65818713153595\n",
            "epochs: 47800 ========> Loss: 0.3348518197597985 ========> Accuracy: 85.66224594372457\n",
            "epochs: 47850 ========> Loss: 0.33484446190816014 ========> Accuracy: 85.66326064677172\n",
            "epochs: 47900 ========> Loss: 0.3348371267382689 ========> Accuracy: 85.65920183458311\n",
            "epochs: 47950 ========> Loss: 0.33482981249603383 ========> Accuracy: 85.66021653763026\n",
            "epochs: 48000 ========> Loss: 0.3348225174306719 ========> Accuracy: 85.66123124067741\n",
            "epochs: 48050 ========> Loss: 0.334815239800244 ========> Accuracy: 85.6571724284888\n",
            "epochs: 48100 ========> Loss: 0.3348079778786302 ========> Accuracy: 85.6551430223945\n",
            "epochs: 48150 ========> Loss: 0.33480072996401256 ========> Accuracy: 85.65209891325304\n",
            "epochs: 48200 ========> Loss: 0.3347934943888932 ========> Accuracy: 85.65108421020588\n",
            "epochs: 48250 ========> Loss: 0.3347862695316294 ========> Accuracy: 85.64905480411159\n",
            "epochs: 48300 ========> Loss: 0.3347790538294062 ========> Accuracy: 85.65209891325304\n",
            "epochs: 48350 ========> Loss: 0.3347718457924876 ========> Accuracy: 85.65209891325304\n",
            "epochs: 48400 ========> Loss: 0.3347646440194876 ========> Accuracy: 85.64499599192297\n",
            "epochs: 48450 ========> Loss: 0.33475744721329503 ========> Accuracy: 85.64195188278151\n",
            "epochs: 48500 ========> Loss: 0.33475025419715715 ========> Accuracy: 85.63586366449857\n",
            "epochs: 48550 ========> Loss: 0.3347430639303011 ========> Accuracy: 85.63383425840428\n",
            "epochs: 48600 ========> Loss: 0.3347358755223495 ========> Accuracy: 85.62977544621566\n",
            "epochs: 48650 ========> Loss: 0.3347286882456863 ========> Accuracy: 85.62774604012135\n",
            "epochs: 48700 ========> Loss: 0.3347215015448683 ========> Accuracy: 85.63180485230997\n",
            "epochs: 48750 ========> Loss: 0.3347143150421721 ========> Accuracy: 85.63281955535712\n",
            "epochs: 48800 ========> Loss: 0.3347071285384341 ========> Accuracy: 85.63484896145142\n",
            "epochs: 48850 ========> Loss: 0.334699942008492 ========> Accuracy: 85.62774604012135\n",
            "epochs: 48900 ========> Loss: 0.33469275559077516 ========> Accuracy: 85.6267313370742\n",
            "epochs: 48950 ========> Loss: 0.33468556957090767 ========> Accuracy: 85.62876074316851\n",
            "epochs: 49000 ========> Loss: 0.33467838435956104 ========> Accuracy: 85.63079014926281\n",
            "epochs: 49050 ========> Loss: 0.33467120046519555 ========> Accuracy: 85.63281955535712\n",
            "epochs: 49100 ========> Loss: 0.3346640184627124 ========> Accuracy: 85.63484896145142\n",
            "epochs: 49150 ========> Loss: 0.3346568389593609 ========> Accuracy: 85.63586366449857\n",
            "epochs: 49200 ========> Loss: 0.3346496625594719 ========> Accuracy: 85.63079014926281\n",
            "epochs: 49250 ========> Loss: 0.33464248982967704 ========> Accuracy: 85.62774604012135\n",
            "epochs: 49300 ========> Loss: 0.3346353212662216 ========> Accuracy: 85.62876074316851\n",
            "epochs: 49350 ========> Loss: 0.3346281572657955 ========> Accuracy: 85.62876074316851\n",
            "epochs: 49400 ========> Loss: 0.3346209981010006 ========> Accuracy: 85.63079014926281\n",
            "epochs: 49450 ========> Loss: 0.3346138439012018 ========> Accuracy: 85.63383425840428\n",
            "epochs: 49500 ========> Loss: 0.33460669463910614 ========> Accuracy: 85.6378930705929\n",
            "epochs: 49550 ========> Loss: 0.33459955012303283 ========> Accuracy: 85.63687836754573\n",
            "epochs: 49600 ========> Loss: 0.3345924099945014 ========> Accuracy: 85.63383425840428\n",
            "epochs: 49650 ========> Loss: 0.33458527373051933 ========> Accuracy: 85.63281955535712\n",
            "epochs: 49700 ========> Loss: 0.33457814064978686 ========> Accuracy: 85.6399224766872\n",
            "epochs: 49750 ========> Loss: 0.3345710099219653 ========> Accuracy: 85.64093717973435\n",
            "epochs: 49800 ========> Loss: 0.3345638805791652 ========> Accuracy: 85.64601069497012\n",
            "epochs: 49850 ========> Loss: 0.33455675152887 ========> Accuracy: 85.64296658582866\n",
            "epochs: 49900 ========> Loss: 0.3345496215676271 ========> Accuracy: 85.64804010106442\n",
            "epochs: 49950 ========> Loss: 0.3345424893949604 ========> Accuracy: 85.64499599192297\n",
            "epochs: 50000 ========> Loss: 0.3345353536270971 ========> Accuracy: 85.64702539801728\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Traning\n",
        "acc, loss, w_1, w_2, w_o = trainMLP(X_train_norm, y_train, w_1, w_2, w_o, 0.0001, 20000)\n",
        "# Plotting Training Loss\n",
        "# plt.plot(loss, label='alpha = 1e-4(1e-4 * ietartion + 1.0)')\n",
        "plt.plot(loss)\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Loss')\n",
        "# plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Posoitive:  11706\n",
            "True Negative:   72700\n",
            "False Posoitive: 4067\n",
            "False Negative:  10078\n",
            "Model Accuracy:  85.64702539801728  %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "85.64702539801728"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training Accuracy\n",
        "ypred_MLP = predictMLP(X_train_norm, w_1, w_2, w_o)\n",
        "ypred_MLP = ypred_MLP > 0.5\n",
        "modelAccuracy(ypred_MLP,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "MHPz0fCR0e3A",
        "outputId": "92bc6cdc-a56e-4173-c0a3-2f53fbd24496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Posoitive:  4903\n",
            "True Negative:   30973\n",
            "False Posoitive: 1846\n",
            "False Negative:  4514\n",
            "Model Accuracy:  84.94175584809167  %\n"
          ]
        }
      ],
      "source": [
        "# Testing Accuracy\n",
        "ypred_MLP = predictMLP(X_test_norm, w_1, w_2, w_o)\n",
        "ypred_MLP = ypred_MLP > 0.5\n",
        "accuracyMLP = modelAccuracy(ypred_MLP,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['FinalModels.mod']"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Saving Model\n",
        "import joblib\n",
        "modelsFilename = 'FinalModels.mod'\n",
        "modelLR = model\n",
        "modelMLP = [w_1, w_2, w_o]\n",
        "models = [modelLR, modelMLP]\n",
        "joblib.dump(models, modelsFilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading Models\n",
        "import joblib\n",
        "modelsLoaded = joblib.load('FinalModels.mod')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "history_visible": true,
      "name": "RainPrediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
